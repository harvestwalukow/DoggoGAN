{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minimal Text-to-Image Generator\n",
    "\n",
    "A lightweight, educational prompt → image model that generates **28×28 grayscale images** from short text prompts.\n",
    "\n",
    "## What This Notebook Demonstrates\n",
    "\n",
    "This notebook builds a complete text-to-image generation pipeline **from scratch**:\n",
    "\n",
    "1. **Synthetic Dataset**: Procedurally generated `(text_prompt, image)` pairs\n",
    "2. **Text Encoder**: Character-level embeddings with mean pooling\n",
    "3. **Image Generator**: MLP decoder that maps latent + noise → image\n",
    "4. **Stochastic Generation**: Same prompt produces different outputs each time\n",
    "\n",
    "## Hard Constraints (Intentional Limitations)\n",
    "\n",
    "- ✅ **PyTorch** only\n",
    "- ✅ **No pretrained models** - everything learned from scratch\n",
    "- ✅ **No diffusion** - direct latent → image mapping\n",
    "- ✅ **No transformers** - simple MLP architecture\n",
    "- ✅ **No CLIP** - custom text encoder\n",
    "- ✅ **No external datasets** - fully synthetic\n",
    "- ✅ **CPU-friendly** - runs on any machine\n",
    "- ✅ **28×28 grayscale** - compact and fast\n",
    "\n",
    "## Architecture Overview\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────┐\n",
    "│                         TEXT ENCODER                            │\n",
    "│  \"circle\" → [c,i,r,c,l,e] → Embeddings → Mean Pool → MLP → z   │\n",
    "│                                                    (64 dims)    │\n",
    "└─────────────────────────────────────────────────────┬───────────┘\n",
    "                                                      │\n",
    "                                                      ▼\n",
    "┌─────────────────────────────────────────────────────────────────┐\n",
    "│                       IMAGE GENERATOR                           │\n",
    "│  [z (64) | noise (32)] → MLP → Sigmoid → 28×28 grayscale       │\n",
    "└─────────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "# Reproducibility (but generation will still be stochastic due to noise injection)\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Device selection - CPU by design for accessibility\n",
    "DEVICE = torch.device('cpu')\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Synthetic Dataset Generation\n",
    "\n",
    "We generate `(text_prompt, image)` pairs procedurally using NumPy.\n",
    "\n",
    "### Design Philosophy\n",
    "- Each prompt maps to a **distribution** of images, not a single template\n",
    "- Randomness in position, size, angle ensures variety\n",
    "- Binary images (0 or 1) for simplicity, stored as float32 for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# IMAGE GENERATION FUNCTIONS\n",
    "# Each function creates a 28x28 numpy array with values in [0, 1]\n",
    "# Randomness is built-in so each call produces a unique image\n",
    "# ============================================================================\n",
    "\n",
    "def generate_horizontal_line():\n",
    "    \"\"\"Horizontal line at random y-position with slight thickness variation.\"\"\"\n",
    "    img = np.zeros((28, 28), dtype=np.float32)\n",
    "    y = np.random.randint(5, 23)  # Keep away from edges\n",
    "    thickness = np.random.randint(1, 3)  # 1-2 pixels thick\n",
    "    x_start = np.random.randint(0, 5)\n",
    "    x_end = np.random.randint(23, 28)\n",
    "    for dy in range(thickness):\n",
    "        if 0 <= y + dy < 28:\n",
    "            img[y + dy, x_start:x_end] = 1.0\n",
    "    return img\n",
    "\n",
    "\n",
    "def generate_vertical_line():\n",
    "    \"\"\"Vertical line at random x-position.\"\"\"\n",
    "    img = np.zeros((28, 28), dtype=np.float32)\n",
    "    x = np.random.randint(5, 23)\n",
    "    thickness = np.random.randint(1, 3)\n",
    "    y_start = np.random.randint(0, 5)\n",
    "    y_end = np.random.randint(23, 28)\n",
    "    for dx in range(thickness):\n",
    "        if 0 <= x + dx < 28:\n",
    "            img[y_start:y_end, x + dx] = 1.0\n",
    "    return img\n",
    "\n",
    "\n",
    "def generate_two_vertical_lines():\n",
    "    \"\"\"Two vertical lines with random spacing.\"\"\"\n",
    "    img = np.zeros((28, 28), dtype=np.float32)\n",
    "    # First line on left half, second on right half\n",
    "    x1 = np.random.randint(4, 10)\n",
    "    x2 = np.random.randint(18, 24)\n",
    "    y_start = np.random.randint(2, 6)\n",
    "    y_end = np.random.randint(22, 26)\n",
    "    img[y_start:y_end, x1] = 1.0\n",
    "    img[y_start:y_end, x2] = 1.0\n",
    "    return img\n",
    "\n",
    "\n",
    "def generate_diagonal_line():\n",
    "    \"\"\"Diagonal line, either top-left to bottom-right or top-right to bottom-left.\"\"\"\n",
    "    img = np.zeros((28, 28), dtype=np.float32)\n",
    "    direction = np.random.choice(['tlbr', 'trbl'])  # Top-left to bottom-right or reverse\n",
    "    offset = np.random.randint(-3, 4)  # Shift the line slightly\n",
    "    \n",
    "    for i in range(28):\n",
    "        if direction == 'tlbr':\n",
    "            x, y = i, i + offset\n",
    "        else:\n",
    "            x, y = i, 27 - i + offset\n",
    "        if 0 <= x < 28 and 0 <= y < 28:\n",
    "            img[y, x] = 1.0\n",
    "            # Add slight thickness\n",
    "            if y + 1 < 28:\n",
    "                img[y + 1, x] = 1.0\n",
    "    return img\n",
    "\n",
    "\n",
    "def generate_cross():\n",
    "    \"\"\"A + or × shape centered with slight position jitter.\"\"\"\n",
    "    img = np.zeros((28, 28), dtype=np.float32)\n",
    "    cx = 14 + np.random.randint(-3, 4)  # Center x with jitter\n",
    "    cy = 14 + np.random.randint(-3, 4)  # Center y with jitter\n",
    "    arm_length = np.random.randint(6, 10)\n",
    "    \n",
    "    # Horizontal arm\n",
    "    for dx in range(-arm_length, arm_length + 1):\n",
    "        if 0 <= cx + dx < 28:\n",
    "            img[cy, cx + dx] = 1.0\n",
    "    \n",
    "    # Vertical arm\n",
    "    for dy in range(-arm_length, arm_length + 1):\n",
    "        if 0 <= cy + dy < 28:\n",
    "            img[cy + dy, cx] = 1.0\n",
    "    \n",
    "    return img\n",
    "\n",
    "\n",
    "def generate_circle():\n",
    "    \"\"\"Circle with random radius and center position.\"\"\"\n",
    "    img = np.zeros((28, 28), dtype=np.float32)\n",
    "    cx = 14 + np.random.randint(-4, 5)\n",
    "    cy = 14 + np.random.randint(-4, 5)\n",
    "    radius = np.random.randint(5, 11)\n",
    "    \n",
    "    # Draw circle using parametric equation\n",
    "    for theta in np.linspace(0, 2 * np.pi, 100):\n",
    "        x = int(cx + radius * np.cos(theta))\n",
    "        y = int(cy + radius * np.sin(theta))\n",
    "        if 0 <= x < 28 and 0 <= y < 28:\n",
    "            img[y, x] = 1.0\n",
    "    \n",
    "    return img\n",
    "\n",
    "\n",
    "def generate_square():\n",
    "    \"\"\"Square outline with random size and position.\"\"\"\n",
    "    img = np.zeros((28, 28), dtype=np.float32)\n",
    "    size = np.random.randint(8, 16)\n",
    "    # Random top-left corner, ensuring square fits\n",
    "    x1 = np.random.randint(2, 28 - size - 2)\n",
    "    y1 = np.random.randint(2, 28 - size - 2)\n",
    "    x2, y2 = x1 + size, y1 + size\n",
    "    \n",
    "    # Draw four sides\n",
    "    img[y1, x1:x2] = 1.0  # Top\n",
    "    img[y2, x1:x2+1] = 1.0  # Bottom\n",
    "    img[y1:y2, x1] = 1.0  # Left\n",
    "    img[y1:y2+1, x2] = 1.0  # Right\n",
    "    \n",
    "    return img\n",
    "\n",
    "\n",
    "def generate_dense_center():\n",
    "    \"\"\"Gaussian-like blob in the center with random spread.\"\"\"\n",
    "    img = np.zeros((28, 28), dtype=np.float32)\n",
    "    cx, cy = 14 + np.random.randint(-2, 3), 14 + np.random.randint(-2, 3)\n",
    "    sigma = np.random.uniform(3, 6)\n",
    "    \n",
    "    for y in range(28):\n",
    "        for x in range(28):\n",
    "            dist_sq = (x - cx) ** 2 + (y - cy) ** 2\n",
    "            img[y, x] = np.exp(-dist_sq / (2 * sigma ** 2))\n",
    "    \n",
    "    # Normalize to [0, 1]\n",
    "    img = img / img.max()\n",
    "    return img\n",
    "\n",
    "\n",
    "def generate_sparse_dots():\n",
    "    \"\"\"5-15 random pixels lit up.\"\"\"\n",
    "    img = np.zeros((28, 28), dtype=np.float32)\n",
    "    n_dots = np.random.randint(5, 16)\n",
    "    \n",
    "    for _ in range(n_dots):\n",
    "        x = np.random.randint(0, 28)\n",
    "        y = np.random.randint(0, 28)\n",
    "        img[y, x] = 1.0\n",
    "    \n",
    "    return img\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# PROMPT TO GENERATOR MAPPING\n",
    "# This dictionary maps text prompts to their image generation functions\n",
    "# ============================================================================\n",
    "\n",
    "PROMPT_GENERATORS = {\n",
    "    \"horizontal line\": generate_horizontal_line,\n",
    "    \"vertical line\": generate_vertical_line,\n",
    "    \"two vertical lines\": generate_two_vertical_lines,\n",
    "    \"diagonal line\": generate_diagonal_line,\n",
    "    \"cross\": generate_cross,\n",
    "    \"circle\": generate_circle,\n",
    "    \"square\": generate_square,\n",
    "    \"dense center\": generate_dense_center,\n",
    "    \"sparse dots\": generate_sparse_dots,\n",
    "}\n",
    "\n",
    "ALL_PROMPTS = list(PROMPT_GENERATORS.keys())\n",
    "print(f\"Available prompts ({len(ALL_PROMPTS)}): {ALL_PROMPTS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# VISUALIZE SAMPLE IMAGES FROM EACH PROMPT\n",
    "# Shows the diversity of images generated for each prompt category\n",
    "# ============================================================================\n",
    "\n",
    "fig, axes = plt.subplots(len(ALL_PROMPTS), 5, figsize=(10, 2 * len(ALL_PROMPTS)))\n",
    "fig.suptitle('Sample Images for Each Prompt (5 variations each)', fontsize=14, fontweight='bold')\n",
    "\n",
    "for row, prompt in enumerate(ALL_PROMPTS):\n",
    "    for col in range(5):\n",
    "        img = PROMPT_GENERATORS[prompt]()\n",
    "        axes[row, col].imshow(img, cmap='gray', vmin=0, vmax=1)\n",
    "        axes[row, col].axis('off')\n",
    "        if col == 0:\n",
    "            axes[row, col].set_title(prompt, fontsize=10, loc='left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PYTORCH DATASET CLASS\n",
    "# Wraps our generators in a PyTorch Dataset for easy batching\n",
    "# ============================================================================\n",
    "\n",
    "class SyntheticTextImageDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Generates (prompt, image) pairs on-the-fly.\n",
    "    \n",
    "    Each __getitem__ call creates a fresh random image for the selected prompt,\n",
    "    ensuring the model sees diverse examples during training.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_samples_per_prompt=1000):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            num_samples_per_prompt: How many times each prompt appears in one epoch\n",
    "        \"\"\"\n",
    "        self.prompts = ALL_PROMPTS\n",
    "        self.num_samples_per_prompt = num_samples_per_prompt\n",
    "        \n",
    "        # Pre-compute the prompt for each index for deterministic iteration order\n",
    "        self.index_to_prompt = []\n",
    "        for prompt in self.prompts:\n",
    "            self.index_to_prompt.extend([prompt] * num_samples_per_prompt)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.index_to_prompt)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        prompt = self.index_to_prompt[idx]\n",
    "        \n",
    "        # Generate a fresh random image for this prompt\n",
    "        img = PROMPT_GENERATORS[prompt]()\n",
    "        \n",
    "        # Convert to tensor: shape (1, 28, 28) for channel dimension\n",
    "        img_tensor = torch.from_numpy(img).unsqueeze(0)\n",
    "        \n",
    "        return prompt, img_tensor\n",
    "\n",
    "\n",
    "# Test the dataset\n",
    "test_dataset = SyntheticTextImageDataset(num_samples_per_prompt=100)\n",
    "print(f\"Dataset size: {len(test_dataset)} samples\")\n",
    "print(f\"Sample: prompt='{test_dataset[0][0]}', image shape={test_dataset[0][1].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Text Encoder\n",
    "\n",
    "We need to convert text prompts into fixed-size vectors that the image generator can understand.\n",
    "\n",
    "### Design Choices\n",
    "- **Character-level tokenization**: Simple and works well for short prompts\n",
    "- **Learned embeddings**: Each character gets a learnable vector\n",
    "- **Mean pooling**: Average all character embeddings to get a fixed-size vector\n",
    "- **MLP projection**: Project pooled embedding to the final latent dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CHARACTER TOKENIZER\n",
    "# Maps characters to integer indices for embedding lookup\n",
    "# ============================================================================\n",
    "\n",
    "class CharacterTokenizer:\n",
    "    \"\"\"\n",
    "    Simple character-level tokenizer.\n",
    "    \n",
    "    Vocabulary: a-z, space, and <PAD> token\n",
    "    All text is lowercased and unknown characters are ignored.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Build vocabulary: PAD=0, space=1, a-z=2-27\n",
    "        self.char_to_idx = {'<PAD>': 0, ' ': 1}\n",
    "        for i, char in enumerate('abcdefghijklmnopqrstuvwxyz'):\n",
    "            self.char_to_idx[char] = i + 2\n",
    "        \n",
    "        self.idx_to_char = {v: k for k, v in self.char_to_idx.items()}\n",
    "        self.vocab_size = len(self.char_to_idx)\n",
    "        self.pad_idx = 0\n",
    "    \n",
    "    def encode(self, text, max_length=32):\n",
    "        \"\"\"\n",
    "        Convert text to list of token indices.\n",
    "        \n",
    "        Args:\n",
    "            text: Input string\n",
    "            max_length: Maximum sequence length (pad/truncate to this)\n",
    "        \n",
    "        Returns:\n",
    "            List of integer token indices\n",
    "        \"\"\"\n",
    "        text = text.lower()\n",
    "        tokens = []\n",
    "        \n",
    "        for char in text:\n",
    "            if char in self.char_to_idx:\n",
    "                tokens.append(self.char_to_idx[char])\n",
    "            # Unknown characters are silently ignored\n",
    "        \n",
    "        # Truncate if too long\n",
    "        tokens = tokens[:max_length]\n",
    "        \n",
    "        # Pad if too short\n",
    "        while len(tokens) < max_length:\n",
    "            tokens.append(self.pad_idx)\n",
    "        \n",
    "        return tokens\n",
    "    \n",
    "    def decode(self, tokens):\n",
    "        \"\"\"Convert token indices back to string.\"\"\"\n",
    "        chars = [self.idx_to_char.get(t, '') for t in tokens if t != self.pad_idx]\n",
    "        return ''.join(chars)\n",
    "\n",
    "\n",
    "# Test the tokenizer\n",
    "tokenizer = CharacterTokenizer()\n",
    "print(f\"Vocabulary size: {tokenizer.vocab_size}\")\n",
    "print(f\"\\nExample encoding:\")\n",
    "test_text = \"circle\"\n",
    "encoded = tokenizer.encode(test_text, max_length=16)\n",
    "print(f\"  '{test_text}' → {encoded[:10]}... (truncated)\")\n",
    "print(f\"  Decoded: '{tokenizer.decode(encoded)}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# TEXT ENCODER NEURAL NETWORK\n",
    "# Converts text prompts into fixed-size latent vectors\n",
    "# ============================================================================\n",
    "\n",
    "class TextEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Encodes text prompts into latent vectors.\n",
    "    \n",
    "    Architecture:\n",
    "        1. Embedding layer: char indices → 32-dim vectors\n",
    "        2. Mean pooling: average over sequence length\n",
    "        3. MLP: project to final latent dimension\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, embed_dim=32, latent_dim=64, max_length=32):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.max_length = max_length\n",
    "        \n",
    "        # Learnable character embeddings\n",
    "        # Each character in the vocabulary gets a embed_dim-dimensional vector\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=vocab_size,\n",
    "            embedding_dim=embed_dim,\n",
    "            padding_idx=0  # PAD tokens get zero vectors\n",
    "        )\n",
    "        \n",
    "        # Project pooled embeddings to latent space\n",
    "        # This MLP learns to extract meaningful features from the averaged embeddings\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(embed_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, latent_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, token_ids):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            token_ids: (batch_size, seq_length) integer tensor\n",
    "        \n",
    "        Returns:\n",
    "            (batch_size, latent_dim) latent vectors\n",
    "        \"\"\"\n",
    "        # Get embeddings: (batch, seq_len, embed_dim)\n",
    "        embeds = self.embedding(token_ids)\n",
    "        \n",
    "        # Create mask for non-padding tokens\n",
    "        # This ensures we only average over actual characters, not PAD tokens\n",
    "        mask = (token_ids != 0).float().unsqueeze(-1)  # (batch, seq_len, 1)\n",
    "        \n",
    "        # Mean pooling: sum embeddings / count of non-pad tokens\n",
    "        # Adding small epsilon to avoid division by zero\n",
    "        masked_embeds = embeds * mask\n",
    "        summed = masked_embeds.sum(dim=1)  # (batch, embed_dim)\n",
    "        counts = mask.sum(dim=1).clamp(min=1)  # (batch, 1)\n",
    "        pooled = summed / counts  # (batch, embed_dim)\n",
    "        \n",
    "        # Project to latent space\n",
    "        latent = self.projection(pooled)  # (batch, latent_dim)\n",
    "        \n",
    "        return latent\n",
    "\n",
    "\n",
    "# Test the text encoder\n",
    "text_encoder = TextEncoder(vocab_size=tokenizer.vocab_size)\n",
    "print(f\"TextEncoder architecture:\")\n",
    "print(text_encoder)\n",
    "\n",
    "# Test forward pass\n",
    "test_tokens = torch.tensor([tokenizer.encode(\"circle\")])\n",
    "test_latent = text_encoder(test_tokens)\n",
    "print(f\"\\nInput shape: {test_tokens.shape}\")\n",
    "print(f\"Output latent shape: {test_latent.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Image Generator\n",
    "\n",
    "The generator takes a text latent vector + random noise and produces a 28×28 image.\n",
    "\n",
    "### Why Add Noise?\n",
    "Without noise, the model would learn a deterministic mapping: same prompt → same image.\n",
    "By concatenating random noise with the latent vector, we enable **stochastic generation**:\n",
    "the same prompt can produce different valid images depending on the noise sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# IMAGE GENERATOR NEURAL NETWORK\n",
    "# Maps latent vectors + noise to 28x28 grayscale images\n",
    "# ============================================================================\n",
    "\n",
    "class ImageGenerator(nn.Module):\n",
    "    \"\"\"\n",
    "    MLP-based image generator.\n",
    "    \n",
    "    Architecture:\n",
    "        Input: latent (64) + noise (32) = 96 dims\n",
    "        → Linear(96, 256) + ReLU + BatchNorm\n",
    "        → Linear(256, 512) + ReLU + BatchNorm\n",
    "        → Linear(512, 784) + Sigmoid\n",
    "        → Reshape to (1, 28, 28)\n",
    "    \n",
    "    The sigmoid ensures output values are in [0, 1] for valid grayscale pixels.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, latent_dim=64, noise_dim=32):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.noise_dim = noise_dim\n",
    "        input_dim = latent_dim + noise_dim\n",
    "        \n",
    "        # MLP decoder\n",
    "        # BatchNorm helps with training stability\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(256, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(512, 784),  # 28 * 28 = 784\n",
    "            nn.Sigmoid()  # Output in [0, 1]\n",
    "        )\n",
    "    \n",
    "    def forward(self, latent, noise=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            latent: (batch_size, latent_dim) text encoding\n",
    "            noise: (batch_size, noise_dim) random noise, or None to sample fresh\n",
    "        \n",
    "        Returns:\n",
    "            (batch_size, 1, 28, 28) generated images\n",
    "        \"\"\"\n",
    "        batch_size = latent.shape[0]\n",
    "        device = latent.device\n",
    "        \n",
    "        # Sample noise if not provided\n",
    "        if noise is None:\n",
    "            noise = torch.randn(batch_size, self.noise_dim, device=device)\n",
    "        \n",
    "        # Concatenate latent and noise\n",
    "        combined = torch.cat([latent, noise], dim=1)\n",
    "        \n",
    "        # Generate flattened image\n",
    "        flat_img = self.decoder(combined)  # (batch, 784)\n",
    "        \n",
    "        # Reshape to image\n",
    "        img = flat_img.view(batch_size, 1, 28, 28)\n",
    "        \n",
    "        return img\n",
    "\n",
    "\n",
    "# Test the generator\n",
    "generator = ImageGenerator()\n",
    "print(f\"ImageGenerator architecture:\")\n",
    "print(generator)\n",
    "\n",
    "# Test forward pass\n",
    "test_output = generator(test_latent)\n",
    "print(f\"\\nInput latent shape: {test_latent.shape}\")\n",
    "print(f\"Output image shape: {test_output.shape}\")\n",
    "print(f\"Output value range: [{test_output.min():.3f}, {test_output.max():.3f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# COMPLETE TEXT-TO-IMAGE MODEL\n",
    "# Combines text encoder and image generator into one module\n",
    "# ============================================================================\n",
    "\n",
    "class TextToImageModel(nn.Module):\n",
    "    \"\"\"\n",
    "    End-to-end text-to-image generation model.\n",
    "    \n",
    "    Pipeline:\n",
    "        text prompt → tokenize → encode → concatenate with noise → generate image\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, embed_dim=32, latent_dim=64, noise_dim=32, max_length=32):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.max_length = max_length\n",
    "        self.noise_dim = noise_dim\n",
    "        \n",
    "        self.text_encoder = TextEncoder(\n",
    "            vocab_size=vocab_size,\n",
    "            embed_dim=embed_dim,\n",
    "            latent_dim=latent_dim,\n",
    "            max_length=max_length\n",
    "        )\n",
    "        \n",
    "        self.image_generator = ImageGenerator(\n",
    "            latent_dim=latent_dim,\n",
    "            noise_dim=noise_dim\n",
    "        )\n",
    "    \n",
    "    def forward(self, token_ids, noise=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            token_ids: (batch_size, seq_length) tokenized prompts\n",
    "            noise: optional noise tensor\n",
    "        \n",
    "        Returns:\n",
    "            (batch_size, 1, 28, 28) generated images\n",
    "        \"\"\"\n",
    "        latent = self.text_encoder(token_ids)\n",
    "        image = self.image_generator(latent, noise)\n",
    "        return image\n",
    "    \n",
    "    def generate(self, prompt, tokenizer, num_samples=1, device='cpu'):\n",
    "        \"\"\"\n",
    "        Convenience method for inference.\n",
    "        \n",
    "        Args:\n",
    "            prompt: string prompt\n",
    "            tokenizer: CharacterTokenizer instance\n",
    "            num_samples: number of images to generate\n",
    "            device: torch device\n",
    "        \n",
    "        Returns:\n",
    "            (num_samples, 1, 28, 28) generated images\n",
    "        \"\"\"\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            # Tokenize and repeat for batch\n",
    "            tokens = tokenizer.encode(prompt, max_length=self.max_length)\n",
    "            token_ids = torch.tensor([tokens] * num_samples, device=device)\n",
    "            \n",
    "            # Generate with fresh noise for each sample\n",
    "            images = self.forward(token_ids)\n",
    "        \n",
    "        return images\n",
    "\n",
    "\n",
    "# Create the full model\n",
    "model = TextToImageModel(vocab_size=tokenizer.vocab_size)\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Training Loop\n",
    "\n",
    "We train the model end-to-end to minimize the difference between generated and target images.\n",
    "\n",
    "### Training Strategy\n",
    "- **Loss**: MSE (Mean Squared Error) - works well for regression to pixel values\n",
    "- **Optimizer**: Adam with default learning rate\n",
    "- **Epochs**: 100 (adjustable based on convergence)\n",
    "- **Batch size**: 64 (balance between speed and gradient quality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# TRAINING CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "# Hyperparameters\n",
    "BATCH_SIZE = 64\n",
    "NUM_EPOCHS = 100\n",
    "LEARNING_RATE = 1e-3\n",
    "SAMPLES_PER_PROMPT = 500  # How many times each prompt appears per epoch\n",
    "\n",
    "# Create dataset and dataloader\n",
    "train_dataset = SyntheticTextImageDataset(num_samples_per_prompt=SAMPLES_PER_PROMPT)\n",
    "\n",
    "# Custom collate function to batch prompts properly\n",
    "def collate_fn(batch):\n",
    "    prompts, images = zip(*batch)\n",
    "    \n",
    "    # Tokenize all prompts\n",
    "    token_ids = torch.tensor([tokenizer.encode(p) for p in prompts])\n",
    "    \n",
    "    # Stack images\n",
    "    images = torch.stack(images)\n",
    "    \n",
    "    return token_ids, images\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=0  # Keep 0 for compatibility\n",
    ")\n",
    "\n",
    "print(f\"Training samples per epoch: {len(train_dataset)}\")\n",
    "print(f\"Batches per epoch: {len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# TRAINING LOOP\n",
    "# ============================================================================\n",
    "\n",
    "# Initialize optimizer and loss function\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Training history for plotting\n",
    "losses = []\n",
    "\n",
    "print(\"Starting training...\\n\")\n",
    "print(f\"{'Epoch':>6} | {'Loss':>10} | {'Progress':>10}\")\n",
    "print(\"-\" * 35)\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    \n",
    "    for batch_idx, (token_ids, target_images) in enumerate(train_loader):\n",
    "        # Move to device\n",
    "        token_ids = token_ids.to(DEVICE)\n",
    "        target_images = target_images.to(DEVICE)\n",
    "        \n",
    "        # Forward pass\n",
    "        # The model samples fresh noise internally\n",
    "        generated_images = model(token_ids)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = criterion(generated_images, target_images)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    # Record average epoch loss\n",
    "    avg_loss = epoch_loss / len(train_loader)\n",
    "    losses.append(avg_loss)\n",
    "    \n",
    "    # Print progress every 10 epochs\n",
    "    if (epoch + 1) % 10 == 0 or epoch == 0:\n",
    "        progress = (epoch + 1) / NUM_EPOCHS * 100\n",
    "        print(f\"{epoch + 1:>6} | {avg_loss:>10.6f} | {progress:>9.1f}%\")\n",
    "\n",
    "print(\"\\nTraining complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PLOT TRAINING LOSS\n",
    "# ============================================================================\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(losses, 'b-', linewidth=2)\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('MSE Loss', fontsize=12)\n",
    "plt.title('Training Loss Over Time', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Final loss: {losses[-1]:.6f}\")\n",
    "print(f\"Best loss: {min(losses):.6f} (epoch {losses.index(min(losses)) + 1})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Sampling & Visualization\n",
    "\n",
    "Now let's see what our model has learned! We'll:\n",
    "1. Generate single images from prompts\n",
    "2. Show multiple generations from the same prompt (demonstrating stochasticity)\n",
    "3. Create a grid comparing all prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# HELPER FUNCTION FOR VISUALIZATION\n",
    "# ============================================================================\n",
    "\n",
    "def visualize_generations(model, tokenizer, prompt, num_samples=8, figsize=(12, 2)):\n",
    "    \"\"\"\n",
    "    Generate and display multiple images from the same prompt.\n",
    "    \n",
    "    This demonstrates the stochastic nature of our generator:\n",
    "    same prompt + different noise = different but valid images.\n",
    "    \"\"\"\n",
    "    images = model.generate(prompt, tokenizer, num_samples=num_samples, device=DEVICE)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, num_samples, figsize=figsize)\n",
    "    fig.suptitle(f'Prompt: \"{prompt}\"', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    for i, ax in enumerate(axes):\n",
    "        img = images[i, 0].cpu().numpy()\n",
    "        ax.imshow(img, cmap='gray', vmin=0, vmax=1)\n",
    "        ax.axis('off')\n",
    "        ax.set_title(f'#{i+1}', fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# DEMO 1: SINGLE PROMPT → MULTIPLE GENERATIONS\n",
    "# Shows that the same prompt produces different outputs each time\n",
    "# ============================================================================\n",
    "\n",
    "print(\"Demonstrating stochastic generation:\")\n",
    "print(\"Same prompt → Different outputs (due to random noise)\\n\")\n",
    "\n",
    "for prompt in [\"circle\", \"horizontal line\", \"cross\", \"square\"]:\n",
    "    visualize_generations(model, tokenizer, prompt, num_samples=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# DEMO 2: GRID OF ALL PROMPTS\n",
    "# Compare generations across all prompt categories\n",
    "# ============================================================================\n",
    "\n",
    "num_samples_per_prompt = 4\n",
    "\n",
    "fig, axes = plt.subplots(\n",
    "    len(ALL_PROMPTS), \n",
    "    num_samples_per_prompt, \n",
    "    figsize=(num_samples_per_prompt * 2, len(ALL_PROMPTS) * 2)\n",
    ")\n",
    "fig.suptitle('Generated Images for All Prompts', fontsize=16, fontweight='bold', y=1.02)\n",
    "\n",
    "for row, prompt in enumerate(ALL_PROMPTS):\n",
    "    images = model.generate(prompt, tokenizer, num_samples=num_samples_per_prompt, device=DEVICE)\n",
    "    \n",
    "    for col in range(num_samples_per_prompt):\n",
    "        img = images[col, 0].cpu().numpy()\n",
    "        axes[row, col].imshow(img, cmap='gray', vmin=0, vmax=1)\n",
    "        axes[row, col].axis('off')\n",
    "        \n",
    "        if col == 0:\n",
    "            axes[row, col].set_ylabel(prompt, fontsize=10, rotation=0, ha='right', va='center')\n",
    "\n",
    "# Adjust layout to show labels\n",
    "plt.subplots_adjust(left=0.15, wspace=0.05, hspace=0.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# DEMO 3: SIDE-BY-SIDE COMPARISON WITH GROUND TRUTH\n",
    "# Compare what the model generates vs. the target distribution\n",
    "# ============================================================================\n",
    "\n",
    "fig, axes = plt.subplots(len(ALL_PROMPTS), 6, figsize=(12, len(ALL_PROMPTS) * 1.5))\n",
    "fig.suptitle('Ground Truth (left 3) vs Generated (right 3)', fontsize=14, fontweight='bold', y=1.02)\n",
    "\n",
    "for row, prompt in enumerate(ALL_PROMPTS):\n",
    "    # Ground truth examples\n",
    "    for col in range(3):\n",
    "        gt_img = PROMPT_GENERATORS[prompt]()\n",
    "        axes[row, col].imshow(gt_img, cmap='gray', vmin=0, vmax=1)\n",
    "        axes[row, col].axis('off')\n",
    "        if col == 0:\n",
    "            axes[row, col].set_title('GT', fontsize=8)\n",
    "    \n",
    "    # Generated examples\n",
    "    generated = model.generate(prompt, tokenizer, num_samples=3, device=DEVICE)\n",
    "    for col in range(3):\n",
    "        img = generated[col, 0].cpu().numpy()\n",
    "        axes[row, col + 3].imshow(img, cmap='gray', vmin=0, vmax=1)\n",
    "        axes[row, col + 3].axis('off')\n",
    "        if col == 0:\n",
    "            axes[row, col + 3].set_title('Gen', fontsize=8)\n",
    "    \n",
    "    # Add prompt label on the left\n",
    "    axes[row, 0].set_ylabel(prompt, fontsize=9, rotation=0, ha='right', va='center')\n",
    "\n",
    "plt.subplots_adjust(left=0.18, wspace=0.05, hspace=0.15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Limitations & Extensions\n",
    "\n",
    "### What This Model CAN Do ✅\n",
    "\n",
    "- Generate recognizable shapes from text prompts\n",
    "- Produce **varied outputs** from the same prompt (stochastic generation)\n",
    "- Run quickly on CPU\n",
    "- Serve as an educational example of text-to-image architecture\n",
    "\n",
    "### What This Model CANNOT Do ❌\n",
    "\n",
    "- Generalize to unseen prompts (it only knows the 9 trained prompts)\n",
    "- Generate complex or realistic images\n",
    "- Understand compositional prompts like \"red circle on the left\"\n",
    "- Produce high-resolution outputs\n",
    "\n",
    "### Why These Limitations Exist\n",
    "\n",
    "1. **Small vocabulary**: Only 9 prompts means limited semantic understanding\n",
    "2. **Character-level encoding**: Can't capture word meanings\n",
    "3. **MLP decoder**: Limited spatial awareness (no convolutions)\n",
    "4. **28×28 resolution**: Inherently low detail\n",
    "5. **No attention mechanism**: Can't focus on relevant parts of the prompt\n",
    "\n",
    "### Possible Extensions (For Further Learning)\n",
    "\n",
    "| Extension | Difficulty | Description |\n",
    "|-----------|------------|-------------|\n",
    "| CNN Decoder | Easy | Replace MLP with transposed convolutions for better spatial structure |\n",
    "| More Prompts | Easy | Add \"triangle\", \"rectangle\", \"three dots\", etc. |\n",
    "| Word Embeddings | Medium | Use word-level instead of character-level tokenization |\n",
    "| Larger Images | Medium | Scale to 64×64 or 128×128 |\n",
    "| Conditioning Injection | Medium | Inject latent at multiple decoder layers |\n",
    "| Attention Mechanism | Hard | Add cross-attention between text and image features |\n",
    "| Diffusion | Hard | Replace direct generation with iterative denoising |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# INTERACTIVE DEMO: TRY YOUR OWN PROMPTS\n",
    "# Note: Only trained prompts will work well!\n",
    "# ============================================================================\n",
    "\n",
    "def interactive_generate(prompt, num_samples=4):\n",
    "    \"\"\"\n",
    "    Generate images from a custom prompt.\n",
    "    \n",
    "    Warning: The model only understands the 9 trained prompts.\n",
    "    Unseen prompts will produce unpredictable results!\n",
    "    \"\"\"\n",
    "    print(f\"Generating {num_samples} images for: '{prompt}'\")\n",
    "    \n",
    "    if prompt not in ALL_PROMPTS:\n",
    "        print(f\"⚠️  Warning: '{prompt}' was not in training data!\")\n",
    "        print(f\"   Trained prompts: {ALL_PROMPTS}\")\n",
    "    \n",
    "    visualize_generations(model, tokenizer, prompt, num_samples=num_samples)\n",
    "\n",
    "\n",
    "# Try some prompts\n",
    "interactive_generate(\"circle\")\n",
    "interactive_generate(\"sparse dots\")\n",
    "\n",
    "# This won't work well - unseen prompt!\n",
    "interactive_generate(\"triangle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SAVE THE MODEL (OPTIONAL)\n",
    "# ============================================================================\n",
    "\n",
    "# Uncomment to save:\n",
    "# torch.save({\n",
    "#     'model_state_dict': model.state_dict(),\n",
    "#     'tokenizer_vocab': tokenizer.char_to_idx,\n",
    "#     'prompts': ALL_PROMPTS,\n",
    "# }, 'text_to_image_model.pt')\n",
    "# print(\"Model saved to 'text_to_image_model.pt'\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"NOTEBOOK COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nSummary:\")\n",
    "print(f\"  - Trained on {len(ALL_PROMPTS)} prompt categories\")\n",
    "print(f\"  - Model size: {total_params:,} parameters\")\n",
    "print(f\"  - Final training loss: {losses[-1]:.6f}\")\n",
    "print(f\"\\nThe model can generate 28×28 grayscale images from text prompts!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
