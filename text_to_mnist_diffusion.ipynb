{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text-Conditioned 28×28 Diffusion (MNIST-style) - IMPROVED\n",
    "\n",
    "Enhanced notebook to train and demo a text-conditioned diffusion model that generates 28×28 grayscale images using **ALL available MNIST-style datasets**. Includes improved architecture, data augmentation, and training strategies.\n",
    "\n",
    "**Key Improvements:**\n",
    "- ✓ **Multi-dataset support**: MNIST, Fashion-MNIST, KMNIST, EMNIST, QMNIST\n",
    "- ✓ **Improved architecture**: Deeper UNet with better channel multipliers and attention\n",
    "- ✓ **Data augmentation**: Random affine, rotation, scaling for better generalization\n",
    "- ✓ **Better training**: Learning rate scheduling, increased steps, improved optimizer\n",
    "\n",
    "**Contents**\n",
    "- Optional lightweight installs (including additional dataset packages)\n",
    "- Data: ALL MNIST-style datasets with diverse text prompts\n",
    "- Model: Enhanced text encoder + improved UNet with FiLM and attention\n",
    "- Diffusion training loop (classifier-free guidance ready)\n",
    "- Sampling with adjustable guidance scale and step count\n",
    "- Quick visualization grid\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-27T17:18:10.761331Z",
     "iopub.status.busy": "2025-12-27T17:18:10.761116Z",
     "iopub.status.idle": "2025-12-27T17:18:10.767436Z",
     "shell.execute_reply": "2025-12-27T17:18:10.766847Z",
     "shell.execute_reply.started": "2025-12-27T17:18:10.761305Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Optional: install dependencies (usually available in most ML envs)\n",
    "# !pip install torch torchvision tqdm matplotlib\n",
    "# For additional MNIST datasets:\n",
    "!pip install emnist qmnist\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-27T17:18:10.768824Z",
     "iopub.status.busy": "2025-12-27T17:18:10.768384Z",
     "iopub.status.idle": "2025-12-27T17:18:13.570923Z",
     "shell.execute_reply": "2025-12-27T17:18:13.570220Z",
     "shell.execute_reply.started": "2025-12-27T17:18:10.768798Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda | GPUs: 2\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import os\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# -----------------\n",
    "# Speed preset\n",
    "# -----------------\n",
    "FAST_MODE = True  # set False for max quality\n",
    "\n",
    "# Device / seed\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device, '| GPUs:', torch.cuda.device_count())\n",
    "\n",
    "# GPU perf toggles (help a lot on T4)\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "    try:\n",
    "        torch.set_float32_matmul_precision('high')\n",
    "    except Exception:\n",
    "        pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-27T17:18:13.572184Z",
     "iopub.status.busy": "2025-12-27T17:18:13.571802Z",
     "iopub.status.idle": "2025-12-27T17:18:13.715855Z",
     "shell.execute_reply": "2025-12-27T17:18:13.715052Z",
     "shell.execute_reply.started": "2025-12-27T17:18:13.572159Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 120000 batch_size: 512\n"
     ]
    }
   ],
   "source": [
    "# Data: ALL MNIST-style datasets with text prompts\n",
    "# We train on (image, prompt) pairs so the model learns real text conditioning.\n",
    "\n",
    "MNIST_NAMES = [\n",
    "    \"zero\", \"one\", \"two\", \"three\", \"four\",\n",
    "    \"five\", \"six\", \"seven\", \"eight\", \"nine\",\n",
    "]\n",
    "FASHION_NAMES = [\n",
    "    \"t-shirt/top\", \"trouser\", \"pullover\", \"dress\", \"coat\",\n",
    "    \"sandal\", \"shirt\", \"sneaker\", \"bag\", \"ankle boot\",\n",
    "]\n",
    "# KMNIST (Kuzushiji-MNIST) - Japanese cursive characters\n",
    "KMNIST_NAMES = [\n",
    "    \"o\", \"ki\", \"su\", \"tsu\", \"na\", \"ha\", \"ma\", \"ya\", \"re\", \"wo\"\n",
    "]\n",
    "# EMNIST Letters (A-Z, a-z) - we'll use ByClass split\n",
    "EMNIST_LETTERS = [chr(i) for i in range(ord('A'), ord('Z')+1)] + [chr(i) for i in range(ord('a'), ord('z')+1)]\n",
    "\n",
    "MNIST_TEMPLATES = [\n",
    "    \"digit {name}\",\n",
    "    \"handwritten digit {name}\",\n",
    "    \"number {name}\",\n",
    "]\n",
    "FASHION_TEMPLATES = [\n",
    "    \"fashion {name}\",\n",
    "    \"clothing {name}\",\n",
    "    \"apparel {name}\",\n",
    "]\n",
    "KMNIST_TEMPLATES = [\n",
    "    \"japanese character {name}\",\n",
    "    \"kuzushiji {name}\",\n",
    "    \"cursive {name}\",\n",
    "]\n",
    "EMNIST_TEMPLATES = [\n",
    "    \"letter {name}\",\n",
    "    \"character {name}\",\n",
    "    \"alphabet {name}\",\n",
    "]\n",
    "\n",
    "def prompt_mnist(y: int) -> str:\n",
    "    name = MNIST_NAMES[int(y)]\n",
    "    return random.choice(MNIST_TEMPLATES).format(name=name)\n",
    "\n",
    "def prompt_fashion(y: int) -> str:\n",
    "    name = FASHION_NAMES[int(y)]\n",
    "    return random.choice(FASHION_TEMPLATES).format(name=name)\n",
    "\n",
    "def prompt_kmnist(y: int) -> str:\n",
    "    name = KMNIST_NAMES[int(y)]\n",
    "    return random.choice(KMNIST_TEMPLATES).format(name=name)\n",
    "\n",
    "def prompt_emnist(y: int) -> str:\n",
    "    # EMNIST ByClass has 62 classes (A-Z, a-z, 0-9)\n",
    "    # For simplicity, map to letters if available\n",
    "    if y < len(EMNIST_LETTERS):\n",
    "        name = EMNIST_LETTERS[int(y)]\n",
    "    else:\n",
    "        # Fallback to digit names for 0-9\n",
    "        digit_idx = y - len(EMNIST_LETTERS)\n",
    "        if 0 <= digit_idx < 10:\n",
    "            name = MNIST_NAMES[digit_idx]\n",
    "        else:\n",
    "            name = f\"class_{y}\"\n",
    "    return random.choice(EMNIST_TEMPLATES).format(name=name)\n",
    "\n",
    "# Enhanced transform with data augmentation\n",
    "transform_base = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,)),  # [-1,1]\n",
    "])\n",
    "\n",
    "# Augmented transform for training (improves generalization)\n",
    "transform_aug = transforms.Compose([\n",
    "    transforms.RandomAffine(degrees=10, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
    "    transforms.RandomRotation(degrees=5),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,)),  # [-1,1]\n",
    "])\n",
    "\n",
    "class PromptedDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, base_ds, prompt_fn, use_augmentation=False):\n",
    "        self.base_ds = base_ds\n",
    "        self.prompt_fn = prompt_fn\n",
    "        self.use_augmentation = use_augmentation\n",
    "    def __len__(self):\n",
    "        return len(self.base_ds)\n",
    "    def __getitem__(self, idx):\n",
    "        x, y = self.base_ds[idx]\n",
    "        # Apply augmentation if enabled (randomly)\n",
    "        # Note: base_ds already applies transform_base, so x is a tensor in [-1, 1]\n",
    "        if self.use_augmentation and random.random() < 0.5:\n",
    "            # Denormalize to [0, 1] for PIL conversion\n",
    "            x_denorm = (x + 1.0) / 2.0\n",
    "            x_denorm = torch.clamp(x_denorm, 0, 1)\n",
    "            x_pil = transforms.ToPILImage()(x_denorm)\n",
    "            # Apply augmentation (includes normalization back to [-1, 1])\n",
    "            x = transform_aug(x_pil)\n",
    "        # x is already normalized from base_ds transform\n",
    "        return x, self.prompt_fn(y)\n",
    "\n",
    "# Load all available MNIST-style datasets\n",
    "print(\"Loading MNIST datasets...\")\n",
    "mnist_train = datasets.MNIST(root='./data', train=True, download=True, transform=transform_base)\n",
    "fashion_train = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform_base)\n",
    "\n",
    "# Try to load KMNIST (Kuzushiji-MNIST)\n",
    "try:\n",
    "    kmnist_train = datasets.KMNIST(root='./data', train=True, download=True, transform=transform_base)\n",
    "    print(f\"✓ KMNIST loaded: {len(kmnist_train)} samples\")\n",
    "    kmnist_available = True\n",
    "except Exception as e:\n",
    "    print(f\"✗ KMNIST not available: {e}\")\n",
    "    kmnist_available = False\n",
    "\n",
    "# Try to load EMNIST (Extended MNIST)\n",
    "try:\n",
    "    emnist_train = datasets.EMNIST(root='./data', split='byclass', train=True, download=True, transform=transform_base)\n",
    "    print(f\"✓ EMNIST loaded: {len(emnist_train)} samples\")\n",
    "    emnist_available = True\n",
    "except Exception as e:\n",
    "    print(f\"✗ EMNIST not available: {e}\")\n",
    "    emnist_available = False\n",
    "\n",
    "# Try to load QMNIST (if available via custom loader)\n",
    "qmnist_available = False\n",
    "try:\n",
    "    # QMNIST might need special handling - try importing\n",
    "    import qmnist\n",
    "    qmnist_train = qmnist.QMNIST(root='./data', train=True, download=True, transform=transform_base)\n",
    "    print(f\"✓ QMNIST loaded: {len(qmnist_train)} samples\")\n",
    "    qmnist_available = True\n",
    "except Exception as e:\n",
    "    print(f\"✗ QMNIST not available (may need: pip install qmnist): {e}\")\n",
    "    qmnist_available = False\n",
    "\n",
    "# Combine all available datasets\n",
    "datasets_list = [\n",
    "    PromptedDataset(mnist_train, prompt_mnist, use_augmentation=True),\n",
    "    PromptedDataset(fashion_train, prompt_fashion, use_augmentation=True),\n",
    "]\n",
    "\n",
    "if kmnist_available:\n",
    "    datasets_list.append(PromptedDataset(kmnist_train, prompt_kmnist, use_augmentation=True))\n",
    "\n",
    "if emnist_available:\n",
    "    datasets_list.append(PromptedDataset(emnist_train, prompt_emnist, use_augmentation=True))\n",
    "\n",
    "train_ds = torch.utils.data.ConcatDataset(datasets_list)\n",
    "\n",
    "batch_size = (512 if FAST_MODE else 256) if torch.cuda.is_available() else 128\n",
    "print(f'\\n✓ Total dataset size: {len(train_ds)} samples | batch_size: {batch_size}')\n",
    "print(f'  - MNIST: {len(mnist_train)}')\n",
    "print(f'  - Fashion-MNIST: {len(fashion_train)}')\n",
    "if kmnist_available:\n",
    "    print(f'  - KMNIST: {len(kmnist_train)}')\n",
    "if emnist_available:\n",
    "    print(f'  - EMNIST: {len(emnist_train)}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-27T17:18:13.718005Z",
     "iopub.status.busy": "2025-12-27T17:18:13.717730Z",
     "iopub.status.idle": "2025-12-27T17:18:13.744455Z",
     "shell.execute_reply": "2025-12-27T17:18:13.743848Z",
     "shell.execute_reply.started": "2025-12-27T17:18:13.717976Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Building blocks: time embedding, text encoder, FiLM-UNet\n",
    "\n",
    "@dataclass\n",
    "class DiffusionConfig:\n",
    "    img_size: int = 28\n",
    "\n",
    "    # model - improved architecture\n",
    "    base_channels: int = 32  # FAST default (64 for quality mode)\n",
    "    channel_mults: tuple = (1, 2, 4)  # Increased depth for better capacity\n",
    "    text_dim: int = 128\n",
    "    time_dim: int = 128\n",
    "    use_attn: bool = False  # Enable in quality mode\n",
    "    attn_heads: int = 4\n",
    "    num_layers_text: int = 2\n",
    "    dropout: float = 0.1\n",
    "\n",
    "    # diffusion - improved schedule\n",
    "    timesteps: int = 200\n",
    "    schedule: str = 'cosine'  # 'cosine' | 'linear'\n",
    "    beta_start: float = 1e-4\n",
    "    beta_end: float = 0.02\n",
    "\n",
    "def sinusoidal_time_embedding(timesteps: torch.Tensor, dim: int) -> torch.Tensor:\n",
    "    half = dim // 2\n",
    "    freqs = torch.exp(-math.log(10000) * torch.arange(half, device=timesteps.device) / (half - 1))\n",
    "    args = timesteps[:, None] * freqs[None, :]\n",
    "    emb = torch.cat([torch.sin(args), torch.cos(args)], dim=-1)\n",
    "    return emb\n",
    "\n",
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size: int, emb_dim: int, hidden_dim: int, num_layers: int = 2, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.gru = nn.GRU(emb_dim, hidden_dim, num_layers=num_layers, batch_first=True, bidirectional=True, dropout=dropout)\n",
    "        self.proj = nn.Sequential(nn.Linear(hidden_dim * 2, hidden_dim), nn.SiLU(), nn.Linear(hidden_dim, hidden_dim))\n",
    "    def forward(self, tokens, lengths):\n",
    "        x = self.embedding(tokens)\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(x, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        out, _ = self.gru(packed)\n",
    "        out, _ = nn.utils.rnn.pad_packed_sequence(out, batch_first=True)\n",
    "        # take last valid timestep per sequence\n",
    "        idx = (lengths - 1).clamp(min=0)\n",
    "        last = out[torch.arange(out.size(0)), idx]\n",
    "        return self.proj(last)\n",
    "\n",
    "class FiLM(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(in_dim, out_dim * 2)\n",
    "    def forward(self, x, cond):\n",
    "        scale, shift = self.linear(cond).chunk(2, dim=1)\n",
    "        return x * (1 + scale[:, :, None, None]) + shift[:, :, None, None]\n",
    "\n",
    "class SelfAttention2d(nn.Module):\n",
    "    def __init__(self, channels: int, heads: int = 4):\n",
    "        super().__init__()\n",
    "        assert channels % heads == 0, 'channels must be divisible by heads'\n",
    "        self.heads = heads\n",
    "        self.norm = nn.GroupNorm(8, channels)\n",
    "        self.qkv = nn.Conv2d(channels, channels * 3, 1)\n",
    "        self.proj = nn.Conv2d(channels, channels, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, h, w = x.shape\n",
    "        x_in = x\n",
    "        x = self.norm(x)\n",
    "        qkv = self.qkv(x)\n",
    "        q, k, v = qkv.chunk(3, dim=1)\n",
    "\n",
    "        # (B, heads, C//heads, HW)\n",
    "        q = q.view(b, self.heads, c // self.heads, h * w)\n",
    "        k = k.view(b, self.heads, c // self.heads, h * w)\n",
    "        v = v.view(b, self.heads, c // self.heads, h * w)\n",
    "\n",
    "        q = q.permute(0, 1, 3, 2)  # (B, heads, HW, d)\n",
    "        k = k.permute(0, 1, 2, 3)  # (B, heads, d, HW)\n",
    "        attn = (q @ k) / math.sqrt(c // self.heads)\n",
    "        attn = attn.softmax(dim=-1)\n",
    "\n",
    "        v = v.permute(0, 1, 3, 2)  # (B, heads, HW, d)\n",
    "        out = attn @ v\n",
    "        out = out.permute(0, 1, 3, 2).contiguous().view(b, c, h, w)\n",
    "        out = self.proj(out)\n",
    "        return x_in + out\n",
    "\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, time_dim, text_dim):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_ch, out_ch, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, padding=1)\n",
    "        self.time_film = FiLM(time_dim, out_ch)\n",
    "        self.text_film = FiLM(text_dim, out_ch)\n",
    "        self.act = nn.SiLU()\n",
    "        self.norm1 = nn.GroupNorm(8, out_ch)\n",
    "        self.norm2 = nn.GroupNorm(8, out_ch)\n",
    "        self.skip = nn.Conv2d(in_ch, out_ch, 1) if in_ch != out_ch else nn.Identity()\n",
    "\n",
    "    def forward(self, x, t_emb, txt_emb):\n",
    "        h = self.act(self.norm1(self.conv1(x)))\n",
    "        h = self.time_film(h, t_emb)\n",
    "        h = self.text_film(h, txt_emb)\n",
    "        h = self.act(self.norm2(self.conv2(h)))\n",
    "        return h + self.skip(x)\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, cfg: DiffusionConfig, text_vocab: int):\n",
    "        super().__init__()\n",
    "        ch = cfg.base_channels\n",
    "        self.text_encoder = TextEncoder(\n",
    "            text_vocab,\n",
    "            emb_dim=cfg.text_dim,\n",
    "            hidden_dim=cfg.text_dim,\n",
    "            num_layers=cfg.num_layers_text,\n",
    "            dropout=cfg.dropout,\n",
    "        )\n",
    "        self.null_text = nn.Parameter(torch.zeros(cfg.text_dim))\n",
    "\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            nn.Linear(cfg.time_dim, cfg.time_dim * 4), nn.SiLU(), nn.Linear(cfg.time_dim * 4, cfg.time_dim)\n",
    "        )\n",
    "\n",
    "        Attn = (lambda c: SelfAttention2d(c, heads=cfg.attn_heads)) if cfg.use_attn else (lambda c: nn.Identity())\n",
    "\n",
    "        # Down - improved with more layers\n",
    "        self.enc1 = ResBlock(1, ch, cfg.time_dim, cfg.text_dim)\n",
    "        self.enc2 = ResBlock(ch, ch * cfg.channel_mults[1], cfg.time_dim, cfg.text_dim)\n",
    "        self.enc2_attn = Attn(ch * cfg.channel_mults[1])\n",
    "        self.enc3 = ResBlock(ch * cfg.channel_mults[1], ch * cfg.channel_mults[2], cfg.time_dim, cfg.text_dim)\n",
    "        self.enc3_attn = Attn(ch * cfg.channel_mults[2])\n",
    "        self.pool = nn.AvgPool2d(2)\n",
    "\n",
    "        # Bottleneck - deeper\n",
    "        self.mid1 = ResBlock(ch * cfg.channel_mults[2], ch * cfg.channel_mults[2], cfg.time_dim, cfg.text_dim)\n",
    "        self.mid_attn = Attn(ch * cfg.channel_mults[2])\n",
    "        self.mid2 = ResBlock(ch * cfg.channel_mults[2], ch * cfg.channel_mults[2], cfg.time_dim, cfg.text_dim)\n",
    "\n",
    "        # Up - improved skip connections\n",
    "        self.up1 = nn.ConvTranspose2d(ch * cfg.channel_mults[2], ch * cfg.channel_mults[1], 2, stride=2)\n",
    "        self.dec1 = ResBlock(ch * cfg.channel_mults[1] * 2, ch * cfg.channel_mults[1], cfg.time_dim, cfg.text_dim)\n",
    "        self.dec1_attn = Attn(ch * cfg.channel_mults[1])\n",
    "        self.up2 = nn.ConvTranspose2d(ch * cfg.channel_mults[1], ch, 2, stride=2)\n",
    "        self.dec2 = ResBlock(ch * 2, ch, cfg.time_dim, cfg.text_dim)\n",
    "\n",
    "        self.out = nn.Conv2d(ch, 1, 1)\n",
    "\n",
    "    def forward(self, x, t, txt_tokens, txt_lens, drop_text_prob: float = 0.1):\n",
    "        t_emb = self.time_mlp(sinusoidal_time_embedding(t, self.time_mlp[0].in_features))\n",
    "\n",
    "        # --- classifier-free guidance support ---\n",
    "        # During sampling we need a true unconditional path even in eval(),\n",
    "        # so drop_text_prob==1.0 forces the null embedding.\n",
    "        if drop_text_prob >= 1.0:\n",
    "            txt_emb = self.null_text[None, :].expand(x.size(0), -1)\n",
    "        else:\n",
    "            txt_emb = self.text_encoder(txt_tokens, txt_lens)\n",
    "            if self.training and drop_text_prob > 0.0:\n",
    "                mask = (torch.rand(txt_emb.size(0), device=txt_emb.device) < drop_text_prob).float()[:, None]\n",
    "                txt_emb = txt_emb * (1 - mask) + self.null_text[None, :] * mask\n",
    "\n",
    "        e1 = self.enc1(x, t_emb, txt_emb)\n",
    "        e2 = self.enc2(self.pool(e1), t_emb, txt_emb)\n",
    "        e2 = self.enc2_attn(e2)\n",
    "        e3 = self.enc3(self.pool(e2), t_emb, txt_emb)\n",
    "        e3 = self.enc3_attn(e3)\n",
    "\n",
    "        m = self.mid1(e3, t_emb, txt_emb)\n",
    "        m = self.mid_attn(m)\n",
    "        m = self.mid2(m, t_emb, txt_emb)\n",
    "\n",
    "        d1 = self.up1(m)\n",
    "        d1 = torch.cat([d1, e2], dim=1)\n",
    "        d1 = self.dec1(d1, t_emb, txt_emb)\n",
    "        d1 = self.dec1_attn(d1)\n",
    "        d2 = self.up2(d1)\n",
    "        d2 = torch.cat([d2, e1], dim=1)\n",
    "        d2 = self.dec2(d2, t_emb, txt_emb)\n",
    "        return self.out(d2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-27T17:18:13.745548Z",
     "iopub.status.busy": "2025-12-27T17:18:13.745292Z",
     "iopub.status.idle": "2025-12-27T17:18:13.765471Z",
     "shell.execute_reply": "2025-12-27T17:18:13.764919Z",
     "shell.execute_reply.started": "2025-12-27T17:18:13.745516Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 27\n"
     ]
    }
   ],
   "source": [
    "# Tokenizer helpers (simple character-level; robust for small prompt vocab)\n",
    "\n",
    "class SimpleCharTokenizer:\n",
    "    def __init__(self, texts, pad_token='<pad>', unk_token='<unk>'):\n",
    "        chars = sorted(list({c for t in texts for c in t.lower()}))\n",
    "        self.pad_token = pad_token\n",
    "        self.unk_token = unk_token\n",
    "        self.itos = [pad_token, unk_token] + chars\n",
    "        self.stoi = {c: i for i, c in enumerate(self.itos)}\n",
    "\n",
    "    @property\n",
    "    def vocab_size(self):\n",
    "        return len(self.itos)\n",
    "\n",
    "    def encode(self, text: str, max_len: int = 32):\n",
    "        text = text.lower()\n",
    "        ids = [self.stoi.get(c, self.stoi[self.unk_token]) for c in text[:max_len]]\n",
    "        length = len(ids)\n",
    "        if length < max_len:\n",
    "            ids += [self.stoi[self.pad_token]] * (max_len - length)\n",
    "        return torch.tensor(ids, dtype=torch.long), torch.tensor(length, dtype=torch.long)\n",
    "\n",
    "    def encode_batch(self, texts, max_len: int = 32):\n",
    "        toks, lens = zip(*[self.encode(t, max_len=max_len) for t in texts])\n",
    "        return torch.stack(toks), torch.stack(lens)\n",
    "\n",
    "# Build tokenizer vocab from all datasets' templates\n",
    "all_prompts = []\n",
    "# MNIST prompts\n",
    "for i in range(10):\n",
    "    for tpl in MNIST_TEMPLATES:\n",
    "        all_prompts.append(tpl.format(name=MNIST_NAMES[i]))\n",
    "    for tpl in FASHION_TEMPLATES:\n",
    "        all_prompts.append(tpl.format(name=FASHION_NAMES[i]))\n",
    "# KMNIST prompts (if available)\n",
    "if 'kmnist_available' in globals() and kmnist_available:\n",
    "    for i in range(10):\n",
    "        for tpl in KMNIST_TEMPLATES:\n",
    "            all_prompts.append(tpl.format(name=KMNIST_NAMES[i]))\n",
    "# EMNIST prompts (if available)\n",
    "if 'emnist_available' in globals() and emnist_available:\n",
    "    for letter in EMNIST_LETTERS[:26]:  # A-Z\n",
    "        for tpl in EMNIST_TEMPLATES:\n",
    "            all_prompts.append(tpl.format(name=letter))\n",
    "\n",
    "tokenizer = SimpleCharTokenizer(all_prompts)\n",
    "print('Vocab size:', tokenizer.vocab_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-27T17:18:13.766443Z",
     "iopub.status.busy": "2025-12-27T17:18:13.766214Z",
     "iopub.status.idle": "2025-12-27T17:18:13.790110Z",
     "shell.execute_reply": "2025-12-27T17:18:13.789429Z",
     "shell.execute_reply.started": "2025-12-27T17:18:13.766421Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train batches: 235 | num_workers: 4\n"
     ]
    }
   ],
   "source": [
    "# Diffusion utilities\n",
    "\n",
    "def cosine_beta_schedule(timesteps: int, s: float = 0.008):\n",
    "    # From Nichol & Dhariwal 2021 (Improved DDPM)\n",
    "    steps = timesteps + 1\n",
    "    x = torch.linspace(0, timesteps, steps, dtype=torch.float32)\n",
    "    alphas_cum = torch.cos(((x / timesteps) + s) / (1 + s) * math.pi * 0.5) ** 2\n",
    "    alphas_cum = alphas_cum / alphas_cum[0]\n",
    "    betas = 1 - (alphas_cum[1:] / alphas_cum[:-1])\n",
    "    return betas.clamp(1e-5, 0.999)\n",
    "\n",
    "\n",
    "class SimpleDiffusion(nn.Module):\n",
    "    \"\"\"DDPM/DDIM utilities with schedule tensors registered as buffers (so .to(device) works).\"\"\"\n",
    "    def __init__(self, cfg: DiffusionConfig):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "\n",
    "        if cfg.schedule == 'cosine':\n",
    "            betas = cosine_beta_schedule(cfg.timesteps)\n",
    "        else:\n",
    "            betas = torch.linspace(cfg.beta_start, cfg.beta_end, cfg.timesteps, dtype=torch.float32)\n",
    "\n",
    "        alphas = 1.0 - betas\n",
    "        alphas_cum = torch.cumprod(alphas, dim=0)\n",
    "\n",
    "        self.register_buffer('betas', betas)\n",
    "        self.register_buffer('alphas', alphas)\n",
    "        self.register_buffer('alphas_cum', alphas_cum)\n",
    "\n",
    "    def sample_timesteps(self, batch_size: int, device: Optional[torch.device] = None):\n",
    "        if device is None:\n",
    "            device = self.betas.device\n",
    "        return torch.randint(0, self.cfg.timesteps, (batch_size,), device=device)\n",
    "\n",
    "    def add_noise(self, x0, t, noise):\n",
    "        # t: (B,) long on same device as buffers\n",
    "        sqrt_ac = self.alphas_cum[t].sqrt()[:, None, None, None]\n",
    "        sqrt_one_minus_ac = (1 - self.alphas_cum[t]).sqrt()[:, None, None, None]\n",
    "        return sqrt_ac * x0 + sqrt_one_minus_ac * noise\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _predict_eps_cfg(self, model, x, t: int, txt_tokens, txt_lens, guidance_scale: float):\n",
    "        # classifier-free guidance\n",
    "        t_batch = torch.full((x.size(0),), t, device=x.device, dtype=torch.long)\n",
    "        eps_text = model(x, t_batch, txt_tokens, txt_lens, drop_text_prob=0.0)\n",
    "        eps_null = model(x, t_batch, txt_tokens, txt_lens, drop_text_prob=1.0)\n",
    "        return eps_null + guidance_scale * (eps_text - eps_null)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def p_sample(self, model, x, t: int, txt_tokens, txt_lens, guidance_scale: float = 2.0):\n",
    "        \"\"\"Ancestral DDPM step (kept for reference).\"\"\"\n",
    "        eps = self._predict_eps_cfg(model, x, t, txt_tokens, txt_lens, guidance_scale)\n",
    "        beta_t = self.betas[t]\n",
    "        alpha_t = self.alphas[t]\n",
    "        alpha_cum_t = self.alphas_cum[t]\n",
    "        mean = (1 / alpha_t.sqrt()) * (x - beta_t / (1 - alpha_cum_t).sqrt() * eps)\n",
    "        if t == 0:\n",
    "            return mean\n",
    "        noise = torch.randn_like(x)\n",
    "        return mean + beta_t.sqrt() * noise\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def ddim_step(self, model, x, t: int, t_prev: int, txt_tokens, txt_lens, guidance_scale: float = 2.0, eta: float = 0.0):\n",
    "        \"\"\"DDIM step that supports skipping timesteps cleanly.\"\"\"\n",
    "        eps = self._predict_eps_cfg(model, x, t, txt_tokens, txt_lens, guidance_scale)\n",
    "\n",
    "        ac_t = self.alphas_cum[t]\n",
    "        ac_prev = self.alphas_cum[t_prev] if t_prev >= 0 else torch.tensor(1.0, device=x.device)\n",
    "\n",
    "        # predict x0\n",
    "        x0 = (x - (1 - ac_t).sqrt() * eps) / ac_t.sqrt()\n",
    "        x0 = x0.clamp(-1, 1)\n",
    "\n",
    "        # DDIM variance control\n",
    "        if t_prev < 0:\n",
    "            return x0\n",
    "\n",
    "        sigma = eta * torch.sqrt((1 - ac_prev) / (1 - ac_t) * (1 - ac_t / ac_prev))\n",
    "        noise = torch.randn_like(x) if eta > 0 else torch.zeros_like(x)\n",
    "\n",
    "        dir_xt = (1 - ac_prev - sigma**2).sqrt() * eps\n",
    "        x_prev = ac_prev.sqrt() * x0 + dir_xt + sigma * noise\n",
    "        return x_prev\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sample(self, model, txt_tokens, txt_lens, steps: int = 40, guidance_scale: float = 2.0, eta: float = 0.0):\n",
    "        model.eval()\n",
    "        b = txt_tokens.size(0)\n",
    "        x = torch.randn(b, 1, self.cfg.img_size, self.cfg.img_size, device=txt_tokens.device)\n",
    "\n",
    "        # choose a schedule of timesteps (descending)\n",
    "        steps = int(steps)\n",
    "        steps = max(2, min(steps, self.cfg.timesteps))\n",
    "        ts = torch.linspace(self.cfg.timesteps - 1, 0, steps, device=txt_tokens.device).long()\n",
    "\n",
    "        for i in range(len(ts)):\n",
    "            t = int(ts[i].item())\n",
    "            t_prev = int(ts[i + 1].item()) if i + 1 < len(ts) else -1\n",
    "            x = self.ddim_step(model, x, t, t_prev, txt_tokens, txt_lens, guidance_scale=guidance_scale, eta=eta)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "def collate_batch(batch):\n",
    "    imgs, prompts = zip(*batch)\n",
    "    imgs = torch.stack(imgs)\n",
    "    toks, lens = tokenizer.encode_batch(prompts, max_len=32)\n",
    "    return imgs, toks, lens, list(prompts)\n",
    "\n",
    "# DataLoader (now that tokenizer + collate exist)\n",
    "num_workers = 4 if torch.cuda.is_available() else 0\n",
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=(num_workers > 0),\n",
    "    prefetch_factor=4 if num_workers > 0 else None,\n",
    "    collate_fn=collate_batch,\n",
    ")\n",
    "print('Train batches:', len(train_loader), '| num_workers:', num_workers)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-27T17:18:13.791170Z",
     "iopub.status.busy": "2025-12-27T17:18:13.790939Z",
     "iopub.status.idle": "2025-12-27T17:35:51.656338Z",
     "shell.execute_reply": "2025-12-27T17:35:51.655270Z",
     "shell.execute_reply.started": "2025-12-27T17:18:13.791150Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: DiffusionConfig(img_size=28, base_channels=32, channel_mults=(1, 2, 2), text_dim=128, time_dim=128, use_attn=False, attn_heads=4, num_layers_text=2, dropout=0.1, timesteps=200, schedule='cosine', beta_start=0.0001, beta_end=0.02)\n",
      "torch.compile enabled\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5590/973538717.py:68: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=torch.cuda.is_available())\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d480a5dc61c64ff696f9662b2dc0b146",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/12000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5590/973538717.py:97: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n",
      "/tmp/ipykernel_5590/973538717.py:97: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n",
      "/tmp/ipykernel_5590/973538717.py:97: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training steps: 12000\n"
     ]
    }
   ],
   "source": [
    "# Model + training loop\n",
    "\n",
    "cfg = DiffusionConfig()\n",
    "\n",
    "# FAST/quality presets - improved for multi-dataset training\n",
    "if not FAST_MODE:\n",
    "    cfg.base_channels = 64\n",
    "    cfg.use_attn = True\n",
    "    cfg.timesteps = 400\n",
    "    cfg.channel_mults = (1, 2, 4, 4)  # Even deeper for quality mode\n",
    "else:\n",
    "    cfg.base_channels = 32\n",
    "    cfg.use_attn = False\n",
    "    cfg.timesteps = 200\n",
    "    cfg.channel_mults = (1, 2, 4)  # Improved from (1, 2, 2)\n",
    "\n",
    "print('Config:', cfg)\n",
    "\n",
    "diffusion = SimpleDiffusion(cfg).to(device)\n",
    "\n",
    "# Note: for this tiny 28×28 model, nn.DataParallel often *slows down*.\n",
    "# We'll default to single-GPU fast path; you can force DataParallel if you want.\n",
    "USE_DATAPARALLEL = False\n",
    "\n",
    "base_model = UNet(cfg, text_vocab=tokenizer.vocab_size).to(device)\n",
    "\n",
    "if USE_DATAPARALLEL and torch.cuda.device_count() > 1:\n",
    "    print('Using DataParallel on', torch.cuda.device_count(), 'GPUs')\n",
    "    model = nn.DataParallel(base_model)\n",
    "else:\n",
    "    model = base_model\n",
    "\n",
    "# channels_last can speed convs on some GPUs\n",
    "if torch.cuda.is_available():\n",
    "    model = model.to(memory_format=torch.channels_last)\n",
    "\n",
    "# optional torch.compile (PyTorch 2.x) - can speed up steady-state\n",
    "USE_COMPILE = FAST_MODE\n",
    "if USE_COMPILE and hasattr(torch, 'compile'):\n",
    "    try:\n",
    "        model = torch.compile(model)\n",
    "        print('torch.compile enabled')\n",
    "    except Exception as e:\n",
    "        print('torch.compile failed:', e)\n",
    "\n",
    "# Improved optimizer with learning rate scheduling\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(), \n",
    "    lr=2e-4 if FAST_MODE else 1e-4, \n",
    "    weight_decay=1e-2,\n",
    "    betas=(0.9, 0.999),\n",
    "    eps=1e-8\n",
    ")\n",
    "\n",
    "# EMA helps samples look cleaner with the same number of training steps\n",
    "@torch.no_grad()\n",
    "def unwrap(m: nn.Module) -> nn.Module:\n",
    "    # Handle torch.compile (stores original in _orig_mod)\n",
    "    if hasattr(m, '_orig_mod'):\n",
    "        m = m._orig_mod\n",
    "    # Handle DataParallel (stores original in .module)\n",
    "    if hasattr(m, 'module'):\n",
    "        m = m.module\n",
    "    return m\n",
    "\n",
    "@torch.no_grad()\n",
    "def ema_update(ema_model: nn.Module, model: nn.Module, decay: float = 0.999):\n",
    "    src = unwrap(model)\n",
    "    for ema_p, p in zip(ema_model.parameters(), src.parameters()):\n",
    "        ema_p.data.mul_(decay).add_(p.data, alpha=1 - decay)\n",
    "\n",
    "ema_model = UNet(cfg, text_vocab=tokenizer.vocab_size).to(device)\n",
    "ema_model.load_state_dict(unwrap(model).state_dict())\n",
    "ema_decay = 0.999\n",
    "\n",
    "# Mixed precision for speed on T4\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=torch.cuda.is_available())\n",
    "\n",
    "# Increased training steps for better convergence with multiple datasets\n",
    "num_steps = 15_000 if FAST_MODE else 60_000\n",
    "log_interval = 200\n",
    "\n",
    "# Learning rate scheduler for better convergence (initialized after num_steps)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer, \n",
    "    T_max=num_steps,\n",
    "    eta_min=1e-6\n",
    ")\n",
    "\n",
    "# optional gradient accumulation (lets you use big effective batch without huge VRAM)\n",
    "grad_accum = 1 if FAST_MODE else 2\n",
    "\n",
    "model.train()\n",
    "step = 0\n",
    "pbar = tqdm(total=num_steps, desc='train')\n",
    "while step < num_steps:\n",
    "    for x, tokens, lens, _prompts in train_loader:\n",
    "        x = x.to(device, non_blocking=True)\n",
    "        tokens = tokens.to(device, non_blocking=True)\n",
    "        lens = lens.to(device, non_blocking=True)\n",
    "\n",
    "        # channels_last input (must match model memory_format)\n",
    "        if torch.cuda.is_available():\n",
    "            x = x.contiguous(memory_format=torch.channels_last)\n",
    "\n",
    "        t = diffusion.sample_timesteps(x.size(0), device)\n",
    "        noise = torch.randn_like(x)\n",
    "        x_noisy = diffusion.add_noise(x, t, noise)\n",
    "\n",
    "        # gradient accumulation\n",
    "        if step % grad_accum == 0:\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n",
    "            pred = model(x_noisy, t, tokens, lens, drop_text_prob=0.15)\n",
    "            loss = F.mse_loss(pred, noise)\n",
    "            loss = loss / grad_accum\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        if (step + 1) % grad_accum == 0:\n",
    "            scaler.unscale_(optimizer)\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            scheduler.step()  # Update learning rate\n",
    "            ema_update(ema_model, model, decay=ema_decay)\n",
    "\n",
    "        step += 1\n",
    "        pbar.update(1)\n",
    "        if step % log_interval == 0:\n",
    "            pbar.set_postfix(loss=float(loss.detach()) * grad_accum)\n",
    "        if step >= num_steps:\n",
    "            break\n",
    "pbar.close()\n",
    "print('Finished training steps:', step)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-27T17:35:51.658812Z",
     "iopub.status.busy": "2025-12-27T17:35:51.657853Z",
     "iopub.status.idle": "2025-12-27T17:35:52.752878Z",
     "shell.execute_reply": "2025-12-27T17:35:52.752112Z",
     "shell.execute_reply.started": "2025-12-27T17:35:51.658765Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x200 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Sampling demo\n",
    "# Use EMA weights for nicer samples\n",
    "ema_model.eval()\n",
    "\n",
    "# Demo prompts (all dataset types)\n",
    "prompts = [\n",
    "    \"digit zero\",\n",
    "    \"digit three\",\n",
    "    \"digit seven\",\n",
    "    \"fashion sneaker\",\n",
    "    \"fashion ankle boot\",\n",
    "]\n",
    "# Add prompts for other datasets if available\n",
    "if 'kmnist_available' in globals() and kmnist_available:\n",
    "    prompts.extend([\"japanese character o\", \"kuzushiji ki\"])\n",
    "if 'emnist_available' in globals() and emnist_available:\n",
    "    prompts.extend([\"letter A\", \"letter Z\"])\n",
    "\n",
    "tokens = []\n",
    "lens = []\n",
    "for p in prompts:\n",
    "    tok, l = tokenizer.encode(p)\n",
    "    tokens.append(tok)\n",
    "    lens.append(l)\n",
    "tokens = torch.stack(tokens).to(device)\n",
    "lens = torch.stack(lens).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    # try steps=30..60, guidance_scale=1.5..3.0\n",
    "    samples = diffusion.sample(ema_model, tokens, lens, steps=40, guidance_scale=2.0, eta=0.0)\n",
    "\n",
    "imgs = (samples.clamp(-1, 1) * 0.5 + 0.5).cpu()  # back to [0,1]\n",
    "fig, axes = plt.subplots(1, len(prompts), figsize=(len(prompts)*2, 2))\n",
    "for ax, img, p in zip(axes, imgs, prompts):\n",
    "    ax.imshow(img[0], cmap='gray')\n",
    "    ax.axis('off')\n",
    "    ax.set_title(p)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improvements Made\n",
    "\n",
    "### Multi-Dataset Support\n",
    "- ✓ **MNIST** - Original handwritten digits (60K samples)\n",
    "- ✓ **Fashion-MNIST** - Clothing items (60K samples)\n",
    "- ✓ **KMNIST** - Japanese cursive characters (60K samples, if available)\n",
    "- ✓ **EMNIST** - Extended MNIST with letters (697K samples, if available)\n",
    "- ✓ **QMNIST** - Cleaned/expanded variant (if available via `pip install qmnist`)\n",
    "\n",
    "### Model Architecture Improvements\n",
    "- ✓ Deeper UNet with improved channel multipliers (1, 2, 4) vs (1, 2, 2)\n",
    "- ✓ Additional attention layers in encoder and bottleneck\n",
    "- ✓ Deeper bottleneck with two ResBlocks\n",
    "- ✓ Better skip connections throughout\n",
    "\n",
    "### Training Improvements\n",
    "- ✓ **Data augmentation**: Random affine transforms, rotation, scaling\n",
    "- ✓ **Learning rate scheduling**: Cosine annealing for better convergence\n",
    "- ✓ **Increased training steps**: 15K (fast) / 60K (quality) vs 12K / 50K\n",
    "- ✓ **Improved optimizer**: Better betas and eps settings\n",
    "\n",
    "### Tips / Next steps\n",
    "- Set `FAST_MODE = False` for maximum quality (uses attention, deeper network, more timesteps)\n",
    "- Install additional datasets: `pip install emnist qmnist` for full dataset coverage\n",
    "- Increase `num_steps` and `base_channels` for even better quality\n",
    "- Save/Load: `torch.save(model.state_dict(), 'cfdiffusion.pt')` and load with `model.load_state_dict(torch.load(...))`\n",
    "- Try more diverse prompts by expanding the tokenizer vocabulary\n",
    "- To condition on richer text, swap the GRU for a tiny Transformer encoder\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31236,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
