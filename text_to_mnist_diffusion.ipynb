{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Text-Conditioned 28×28 Diffusion (MNIST-style)\n",
        "\n",
        "Minimal notebook to train and demo a tiny text-conditioned diffusion model that generates 28×28 grayscale images (MNIST style). Intended for fast iteration and interview demos; keep batch/steps small for a quick run, or bump them for quality.\n",
        "\n",
        "**Contents**\n",
        "- Optional lightweight installs\n",
        "- Data: MNIST + simple text prompts\n",
        "- Model: small text encoder + UNet with FiLM\n",
        "- Diffusion training loop (classifier-free guidance ready)\n",
        "- Sampling with adjustable guidance scale and step count\n",
        "- Quick visualization grid\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional: install dependencies (usually available in most ML envs)\n",
        "# !pip install torch torchvision tqdm matplotlib\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import math\n",
        "import random\n",
        "from dataclasses import dataclass\n",
        "from typing import Optional\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# Device / seed\n",
        "seed = 42\n",
        "random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device:', device)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data: MNIST + FashionMNIST with text prompts\n",
        "# We train on (image, prompt) pairs so the model learns real text conditioning.\n",
        "\n",
        "MNIST_NAMES = [\n",
        "    \"zero\", \"one\", \"two\", \"three\", \"four\",\n",
        "    \"five\", \"six\", \"seven\", \"eight\", \"nine\",\n",
        "]\n",
        "FASHION_NAMES = [\n",
        "    \"t-shirt/top\", \"trouser\", \"pullover\", \"dress\", \"coat\",\n",
        "    \"sandal\", \"shirt\", \"sneaker\", \"bag\", \"ankle boot\",\n",
        "]\n",
        "\n",
        "MNIST_TEMPLATES = [\n",
        "    \"digit {name}\",\n",
        "    \"handwritten digit {name}\",\n",
        "]\n",
        "FASHION_TEMPLATES = [\n",
        "    \"fashion {name}\",\n",
        "    \"clothing {name}\",\n",
        "]\n",
        "\n",
        "def prompt_mnist(y: int) -> str:\n",
        "    name = MNIST_NAMES[int(y)]\n",
        "    return random.choice(MNIST_TEMPLATES).format(name=name)\n",
        "\n",
        "def prompt_fashion(y: int) -> str:\n",
        "    name = FASHION_NAMES[int(y)]\n",
        "    return random.choice(FASHION_TEMPLATES).format(name=name)\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,)),  # [-1,1]\n",
        "])\n",
        "\n",
        "class PromptedDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, base_ds, prompt_fn):\n",
        "        self.base_ds = base_ds\n",
        "        self.prompt_fn = prompt_fn\n",
        "    def __len__(self):\n",
        "        return len(self.base_ds)\n",
        "    def __getitem__(self, idx):\n",
        "        x, y = self.base_ds[idx]\n",
        "        return x, self.prompt_fn(y)\n",
        "\n",
        "mnist_train = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "fashion_train = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
        "\n",
        "use_dataset = 'both'  # 'mnist' | 'fashion' | 'both'\n",
        "if use_dataset == 'mnist':\n",
        "    train_ds = PromptedDataset(mnist_train, prompt_mnist)\n",
        "elif use_dataset == 'fashion':\n",
        "    train_ds = PromptedDataset(fashion_train, prompt_fashion)\n",
        "else:\n",
        "    train_ds = torch.utils.data.ConcatDataset([\n",
        "        PromptedDataset(mnist_train, prompt_mnist),\n",
        "        PromptedDataset(fashion_train, prompt_fashion),\n",
        "    ])\n",
        "\n",
        "batch_size = 256 if torch.cuda.is_available() else 128\n",
        "print('Dataset size:', len(train_ds), 'batch_size:', batch_size)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Building blocks: time embedding, text encoder, FiLM-UNet\n",
        "\n",
        "@dataclass\n",
        "class DiffusionConfig:\n",
        "    img_size: int = 28\n",
        "    base_channels: int = 32\n",
        "    channel_mults: tuple = (1, 2, 2)\n",
        "    text_dim: int = 128\n",
        "    time_dim: int = 128\n",
        "    num_heads: int = 4\n",
        "    num_layers_text: int = 2\n",
        "    dropout: float = 0.1\n",
        "    timesteps: int = 400\n",
        "    beta_start: float = 1e-4\n",
        "    beta_end: float = 0.02\n",
        "\n",
        "def sinusoidal_time_embedding(timesteps: torch.Tensor, dim: int) -> torch.Tensor:\n",
        "    half = dim // 2\n",
        "    freqs = torch.exp(-math.log(10000) * torch.arange(half, device=timesteps.device) / (half - 1))\n",
        "    args = timesteps[:, None] * freqs[None, :]\n",
        "    emb = torch.cat([torch.sin(args), torch.cos(args)], dim=-1)\n",
        "    return emb\n",
        "\n",
        "class TextEncoder(nn.Module):\n",
        "    def __init__(self, vocab_size: int, emb_dim: int, hidden_dim: int, num_layers: int = 2, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, emb_dim)\n",
        "        self.gru = nn.GRU(emb_dim, hidden_dim, num_layers=num_layers, batch_first=True, bidirectional=True, dropout=dropout)\n",
        "        self.proj = nn.Sequential(nn.Linear(hidden_dim * 2, hidden_dim), nn.SiLU(), nn.Linear(hidden_dim, hidden_dim))\n",
        "    def forward(self, tokens, lengths):\n",
        "        x = self.embedding(tokens)\n",
        "        packed = nn.utils.rnn.pack_padded_sequence(x, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
        "        out, _ = self.gru(packed)\n",
        "        out, _ = nn.utils.rnn.pad_packed_sequence(out, batch_first=True)\n",
        "        # take last valid timestep per sequence\n",
        "        idx = (lengths - 1).clamp(min=0)\n",
        "        last = out[torch.arange(out.size(0)), idx]\n",
        "        return self.proj(last)\n",
        "\n",
        "class FiLM(nn.Module):\n",
        "    def __init__(self, in_dim, out_dim):\n",
        "        super().__init__()\n",
        "        self.linear = nn.Linear(in_dim, out_dim * 2)\n",
        "    def forward(self, x, cond):\n",
        "        scale, shift = self.linear(cond).chunk(2, dim=1)\n",
        "        return x * (1 + scale[:, :, None, None]) + shift[:, :, None, None]\n",
        "\n",
        "class ResBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, time_dim, text_dim):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(in_ch, out_ch, 3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, padding=1)\n",
        "        self.time_film = FiLM(time_dim, out_ch)\n",
        "        self.text_film = FiLM(text_dim, out_ch)\n",
        "        self.act = nn.SiLU()\n",
        "        self.norm1 = nn.GroupNorm(8, out_ch)\n",
        "        self.norm2 = nn.GroupNorm(8, out_ch)\n",
        "        self.skip = nn.Conv2d(in_ch, out_ch, 1) if in_ch != out_ch else nn.Identity()\n",
        "    def forward(self, x, t_emb, txt_emb):\n",
        "        h = self.act(self.norm1(self.conv1(x)))\n",
        "        h = self.time_film(h, t_emb)\n",
        "        h = self.text_film(h, txt_emb)\n",
        "        h = self.act(self.norm2(self.conv2(h)))\n",
        "        return h + self.skip(x)\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, cfg: DiffusionConfig, text_vocab: int):\n",
        "        super().__init__()\n",
        "        ch = cfg.base_channels\n",
        "        self.text_encoder = TextEncoder(\n",
        "            text_vocab,\n",
        "            emb_dim=cfg.text_dim,\n",
        "            hidden_dim=cfg.text_dim,\n",
        "            num_layers=cfg.num_layers_text,\n",
        "            dropout=cfg.dropout,\n",
        "        )\n",
        "        self.null_text = nn.Parameter(torch.zeros(cfg.text_dim))\n",
        "\n",
        "        self.time_mlp = nn.Sequential(\n",
        "            nn.Linear(cfg.time_dim, cfg.time_dim * 4), nn.SiLU(), nn.Linear(cfg.time_dim * 4, cfg.time_dim)\n",
        "        )\n",
        "\n",
        "        # Down\n",
        "        self.enc1 = ResBlock(1, ch, cfg.time_dim, cfg.text_dim)\n",
        "        self.enc2 = ResBlock(ch, ch * cfg.channel_mults[1], cfg.time_dim, cfg.text_dim)\n",
        "        self.enc3 = ResBlock(ch * cfg.channel_mults[1], ch * cfg.channel_mults[2], cfg.time_dim, cfg.text_dim)\n",
        "        self.pool = nn.AvgPool2d(2)\n",
        "\n",
        "        # Bottleneck\n",
        "        self.mid = ResBlock(ch * cfg.channel_mults[2], ch * cfg.channel_mults[2], cfg.time_dim, cfg.text_dim)\n",
        "\n",
        "        # Up\n",
        "        self.up1 = nn.ConvTranspose2d(ch * cfg.channel_mults[2], ch * cfg.channel_mults[1], 2, stride=2)\n",
        "        self.dec1 = ResBlock(ch * cfg.channel_mults[1] * 2, ch * cfg.channel_mults[1], cfg.time_dim, cfg.text_dim)\n",
        "        self.up2 = nn.ConvTranspose2d(ch * cfg.channel_mults[1], ch, 2, stride=2)\n",
        "        self.dec2 = ResBlock(ch * 2, ch, cfg.time_dim, cfg.text_dim)\n",
        "\n",
        "        self.out = nn.Conv2d(ch, 1, 1)\n",
        "\n",
        "    def forward(self, x, t, txt_tokens, txt_lens, drop_text_prob: float = 0.1):\n",
        "        t_emb = self.time_mlp(sinusoidal_time_embedding(t, self.time_mlp[0].in_features))\n",
        "\n",
        "        # --- classifier-free guidance support ---\n",
        "        # During sampling we need a true unconditional path even in eval(),\n",
        "        # so drop_text_prob==1.0 forces the null embedding.\n",
        "        if drop_text_prob >= 1.0:\n",
        "            txt_emb = self.null_text[None, :].expand(x.size(0), -1)\n",
        "        else:\n",
        "            txt_emb = self.text_encoder(txt_tokens, txt_lens)\n",
        "            if self.training and drop_text_prob > 0.0:\n",
        "                mask = (torch.rand(txt_emb.size(0), device=txt_emb.device) < drop_text_prob).float()[:, None]\n",
        "                txt_emb = txt_emb * (1 - mask) + self.null_text[None, :] * mask\n",
        "\n",
        "        e1 = self.enc1(x, t_emb, txt_emb)\n",
        "        e2 = self.enc2(self.pool(e1), t_emb, txt_emb)\n",
        "        e3 = self.enc3(self.pool(e2), t_emb, txt_emb)\n",
        "\n",
        "        m = self.mid(e3, t_emb, txt_emb)\n",
        "\n",
        "        d1 = self.up1(m)\n",
        "        d1 = torch.cat([d1, e2], dim=1)\n",
        "        d1 = self.dec1(d1, t_emb, txt_emb)\n",
        "        d2 = self.up2(d1)\n",
        "        d2 = torch.cat([d2, e1], dim=1)\n",
        "        d2 = self.dec2(d2, t_emb, txt_emb)\n",
        "        return self.out(d2)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Tokenizer helpers (simple character-level; robust for small prompt vocab)\n",
        "\n",
        "class SimpleCharTokenizer:\n",
        "    def __init__(self, texts, pad_token='<pad>', unk_token='<unk>'):\n",
        "        chars = sorted(list({c for t in texts for c in t.lower()}))\n",
        "        self.pad_token = pad_token\n",
        "        self.unk_token = unk_token\n",
        "        self.itos = [pad_token, unk_token] + chars\n",
        "        self.stoi = {c: i for i, c in enumerate(self.itos)}\n",
        "\n",
        "    @property\n",
        "    def vocab_size(self):\n",
        "        return len(self.itos)\n",
        "\n",
        "    def encode(self, text: str, max_len: int = 32):\n",
        "        text = text.lower()\n",
        "        ids = [self.stoi.get(c, self.stoi[self.unk_token]) for c in text[:max_len]]\n",
        "        length = len(ids)\n",
        "        if length < max_len:\n",
        "            ids += [self.stoi[self.pad_token]] * (max_len - length)\n",
        "        return torch.tensor(ids, dtype=torch.long), torch.tensor(length, dtype=torch.long)\n",
        "\n",
        "    def encode_batch(self, texts, max_len: int = 32):\n",
        "        toks, lens = zip(*[self.encode(t, max_len=max_len) for t in texts])\n",
        "        return torch.stack(toks), torch.stack(lens)\n",
        "\n",
        "# Build tokenizer vocab from both datasets' templates\n",
        "all_prompts = []\n",
        "for i in range(10):\n",
        "    for tpl in MNIST_TEMPLATES:\n",
        "        all_prompts.append(tpl.format(name=MNIST_NAMES[i]))\n",
        "    for tpl in FASHION_TEMPLATES:\n",
        "        all_prompts.append(tpl.format(name=FASHION_NAMES[i]))\n",
        "\n",
        "tokenizer = SimpleCharTokenizer(all_prompts)\n",
        "print('Vocab size:', tokenizer.vocab_size)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Diffusion utilities\n",
        "\n",
        "class SimpleDiffusion(nn.Module):\n",
        "    \"\"\"DDPM utilities with schedule tensors registered as buffers (so .to(device) works).\"\"\"\n",
        "    def __init__(self, cfg: DiffusionConfig):\n",
        "        super().__init__()\n",
        "        self.cfg = cfg\n",
        "\n",
        "        betas = torch.linspace(cfg.beta_start, cfg.beta_end, cfg.timesteps, dtype=torch.float32)\n",
        "        alphas = 1.0 - betas\n",
        "        alphas_cum = torch.cumprod(alphas, dim=0)\n",
        "\n",
        "        self.register_buffer('betas', betas)\n",
        "        self.register_buffer('alphas', alphas)\n",
        "        self.register_buffer('alphas_cum', alphas_cum)\n",
        "\n",
        "    def sample_timesteps(self, batch_size: int, device: Optional[torch.device] = None):\n",
        "        if device is None:\n",
        "            device = self.betas.device\n",
        "        return torch.randint(0, self.cfg.timesteps, (batch_size,), device=device)\n",
        "\n",
        "    def add_noise(self, x0, t, noise):\n",
        "        # t: (B,) long on same device as buffers\n",
        "        sqrt_ac = self.alphas_cum[t].sqrt()[:, None, None, None]\n",
        "        sqrt_one_minus_ac = (1 - self.alphas_cum[t]).sqrt()[:, None, None, None]\n",
        "        return sqrt_ac * x0 + sqrt_one_minus_ac * noise\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def _predict_eps_cfg(self, model, x, t: int, txt_tokens, txt_lens, guidance_scale: float):\n",
        "        # classifier-free guidance\n",
        "        t_batch = torch.full((x.size(0),), t, device=x.device, dtype=torch.long)\n",
        "        eps_text = model(x, t_batch, txt_tokens, txt_lens, drop_text_prob=0.0)\n",
        "        eps_null = model(x, t_batch, txt_tokens, txt_lens, drop_text_prob=1.0)\n",
        "        return eps_null + guidance_scale * (eps_text - eps_null)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def p_sample(self, model, x, t: int, txt_tokens, txt_lens, guidance_scale: float = 2.0):\n",
        "        \"\"\"Ancestral DDPM step (kept for reference).\"\"\"\n",
        "        eps = self._predict_eps_cfg(model, x, t, txt_tokens, txt_lens, guidance_scale)\n",
        "        beta_t = self.betas[t]\n",
        "        alpha_t = self.alphas[t]\n",
        "        alpha_cum_t = self.alphas_cum[t]\n",
        "        mean = (1 / alpha_t.sqrt()) * (x - beta_t / (1 - alpha_cum_t).sqrt() * eps)\n",
        "        if t == 0:\n",
        "            return mean\n",
        "        noise = torch.randn_like(x)\n",
        "        return mean + beta_t.sqrt() * noise\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def ddim_step(self, model, x, t: int, t_prev: int, txt_tokens, txt_lens, guidance_scale: float = 2.0, eta: float = 0.0):\n",
        "        \"\"\"DDIM step that supports skipping timesteps cleanly.\"\"\"\n",
        "        eps = self._predict_eps_cfg(model, x, t, txt_tokens, txt_lens, guidance_scale)\n",
        "\n",
        "        ac_t = self.alphas_cum[t]\n",
        "        ac_prev = self.alphas_cum[t_prev] if t_prev >= 0 else torch.tensor(1.0, device=x.device)\n",
        "\n",
        "        # predict x0\n",
        "        x0 = (x - (1 - ac_t).sqrt() * eps) / ac_t.sqrt()\n",
        "        x0 = x0.clamp(-1, 1)\n",
        "\n",
        "        # DDIM variance control\n",
        "        if t_prev < 0:\n",
        "            return x0\n",
        "\n",
        "        sigma = eta * torch.sqrt((1 - ac_prev) / (1 - ac_t) * (1 - ac_t / ac_prev))\n",
        "        noise = torch.randn_like(x) if eta > 0 else torch.zeros_like(x)\n",
        "\n",
        "        dir_xt = (1 - ac_prev - sigma**2).sqrt() * eps\n",
        "        x_prev = ac_prev.sqrt() * x0 + dir_xt + sigma * noise\n",
        "        return x_prev\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def sample(self, model, txt_tokens, txt_lens, steps: int = 40, guidance_scale: float = 2.0, eta: float = 0.0):\n",
        "        model.eval()\n",
        "        b = txt_tokens.size(0)\n",
        "        x = torch.randn(b, 1, self.cfg.img_size, self.cfg.img_size, device=txt_tokens.device)\n",
        "\n",
        "        # choose a schedule of timesteps (descending)\n",
        "        steps = int(steps)\n",
        "        steps = max(2, min(steps, self.cfg.timesteps))\n",
        "        ts = torch.linspace(self.cfg.timesteps - 1, 0, steps, device=txt_tokens.device).long()\n",
        "\n",
        "        for i in range(len(ts)):\n",
        "            t = int(ts[i].item())\n",
        "            t_prev = int(ts[i + 1].item()) if i + 1 < len(ts) else -1\n",
        "            x = self.ddim_step(model, x, t, t_prev, txt_tokens, txt_lens, guidance_scale=guidance_scale, eta=eta)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "def collate_batch(batch):\n",
        "    imgs, prompts = zip(*batch)\n",
        "    imgs = torch.stack(imgs)\n",
        "    toks, lens = tokenizer.encode_batch(prompts, max_len=32)\n",
        "    return imgs, toks, lens, list(prompts)\n",
        "\n",
        "# DataLoader (now that tokenizer + collate exist)\n",
        "train_loader = DataLoader(\n",
        "    train_ds,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=2,\n",
        "    pin_memory=True,\n",
        "    collate_fn=collate_batch,\n",
        ")\n",
        "print('Train batches:', len(train_loader))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model + training loop\n",
        "\n",
        "cfg = DiffusionConfig()\n",
        "diffusion = SimpleDiffusion(cfg).to(device)\n",
        "\n",
        "base_model = UNet(cfg, text_vocab=tokenizer.vocab_size).to(device)\n",
        "\n",
        "# Use both T4s on Kaggle if available\n",
        "if torch.cuda.device_count() > 1:\n",
        "    print('Using DataParallel on', torch.cuda.device_count(), 'GPUs')\n",
        "    model = nn.DataParallel(base_model)\n",
        "else:\n",
        "    model = base_model\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-4, weight_decay=1e-2)\n",
        "\n",
        "# EMA helps samples look cleaner with the same number of training steps\n",
        "@torch.no_grad()\n",
        "def unwrap(m: nn.Module) -> nn.Module:\n",
        "    return m.module if hasattr(m, 'module') else m\n",
        "\n",
        "@torch.no_grad()\n",
        "def ema_update(ema_model: nn.Module, model: nn.Module, decay: float = 0.999):\n",
        "    src = unwrap(model)\n",
        "    for ema_p, p in zip(ema_model.parameters(), src.parameters()):\n",
        "        ema_p.data.mul_(decay).add_(p.data, alpha=1 - decay)\n",
        "\n",
        "ema_model = UNet(cfg, text_vocab=tokenizer.vocab_size).to(device)\n",
        "ema_model.load_state_dict(unwrap(model).state_dict())\n",
        "ema_decay = 0.999\n",
        "\n",
        "# Mixed precision for speed on T4\n",
        "scaler = torch.cuda.amp.GradScaler(enabled=torch.cuda.is_available())\n",
        "\n",
        "num_steps = 30_000  # \"complete\" baseline for MNIST/FashionMNIST\n",
        "log_interval = 200\n",
        "\n",
        "model.train()\n",
        "step = 0\n",
        "pbar = tqdm(total=num_steps, desc='train')\n",
        "while step < num_steps:\n",
        "    for x, tokens, lens, _prompts in train_loader:\n",
        "        x = x.to(device, non_blocking=True)\n",
        "        tokens = tokens.to(device, non_blocking=True)\n",
        "        lens = lens.to(device, non_blocking=True)\n",
        "\n",
        "        t = diffusion.sample_timesteps(x.size(0), device)\n",
        "        noise = torch.randn_like(x)\n",
        "        x_noisy = diffusion.add_noise(x, t, noise)\n",
        "\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n",
        "            pred = model(x_noisy, t, tokens, lens, drop_text_prob=0.15)\n",
        "            loss = F.mse_loss(pred, noise)\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.unscale_(optimizer)\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        ema_update(ema_model, model, decay=ema_decay)\n",
        "\n",
        "        step += 1\n",
        "        pbar.update(1)\n",
        "        if step % log_interval == 0:\n",
        "            pbar.set_postfix(loss=float(loss.detach()))\n",
        "        if step >= num_steps:\n",
        "            break\n",
        "pbar.close()\n",
        "print('Finished training steps:', step)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sampling demo\n",
        "# Use EMA weights for nicer samples\n",
        "ema_model.eval()\n",
        "\n",
        "# Demo prompts (digits + fashion)\n",
        "prompts = [\n",
        "    \"digit zero\",\n",
        "    \"digit three\",\n",
        "    \"digit seven\",\n",
        "    \"fashion sneaker\",\n",
        "    \"fashion ankle boot\",\n",
        "]\n",
        "\n",
        "tokens = []\n",
        "lens = []\n",
        "for p in prompts:\n",
        "    tok, l = tokenizer.encode(p)\n",
        "    tokens.append(tok)\n",
        "    lens.append(l)\n",
        "tokens = torch.stack(tokens).to(device)\n",
        "lens = torch.stack(lens).to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    # try steps=30..60, guidance_scale=1.5..3.0\n",
        "    samples = diffusion.sample(ema_model, tokens, lens, steps=40, guidance_scale=2.0, eta=0.0)\n",
        "\n",
        "imgs = (samples.clamp(-1, 1) * 0.5 + 0.5).cpu()  # back to [0,1]\n",
        "fig, axes = plt.subplots(1, len(prompts), figsize=(len(prompts)*2, 2))\n",
        "for ax, img, p in zip(axes, imgs, prompts):\n",
        "    ax.imshow(img[0], cmap='gray')\n",
        "    ax.axis('off')\n",
        "    ax.set_title(p)\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tips / Next steps\n",
        "- Increase `num_steps`, `base_channels`, and train for more iterations for better quality.\n",
        "- Try more diverse prompts by expanding the tokenizer vocabulary (add your own prompt list).\n",
        "- Save/Load: `torch.save(model.state_dict(), 'cfdiffusion.pt')` and load with `model.load_state_dict(torch.load(...))`.\n",
        "- Swap sampler stride logic to use full step schedule for best results; current stride ties `steps` to coarse skipping for speed.\n",
        "- To condition on richer text, swap the GRU for a tiny Transformer encoder.\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
