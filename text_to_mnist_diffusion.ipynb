{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text-Conditioned 28×28 Diffusion (MNIST-style) - IMPROVED\n",
    "\n",
    "Enhanced notebook to train and demo a text-conditioned diffusion model that generates 28×28 grayscale images using **ALL available MNIST-style datasets**. Includes improved architecture, data augmentation, and training strategies.\n",
    "\n",
    "**Key Improvements:**\n",
    "- ✓ **Multi-dataset support**: MNIST, Fashion-MNIST, KMNIST, EMNIST, QMNIST\n",
    "- ✓ **Improved architecture**: Deeper UNet with better channel multipliers and attention\n",
    "- ✓ **Data augmentation**: Random affine, rotation, scaling for better generalization\n",
    "- ✓ **Better training**: Learning rate scheduling, increased steps, improved optimizer\n",
    "\n",
    "**Contents**\n",
    "- Optional lightweight installs (including additional dataset packages)\n",
    "- Data: ALL MNIST-style datasets with diverse text prompts\n",
    "- Model: Enhanced text encoder + improved UNet with FiLM and attention\n",
    "- Diffusion training loop (classifier-free guidance ready)\n",
    "- Sampling with adjustable guidance scale and step count\n",
    "- Quick visualization grid\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T09:10:30.504860Z",
     "iopub.status.busy": "2026-01-16T09:10:30.504629Z",
     "iopub.status.idle": "2026-01-16T09:10:33.496752Z",
     "shell.execute_reply": "2026-01-16T09:10:33.495800Z",
     "shell.execute_reply.started": "2026-01-16T09:10:30.504837Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting emnist\n",
      "  Downloading emnist-0.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement qmnist (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for qmnist\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Optional: install dependencies (usually available in most ML envs)\n",
    "# !pip install torch torchvision tqdm matplotlib\n",
    "# For additional MNIST datasets:\n",
    "!pip install emnist qmnist\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T09:10:33.498423Z",
     "iopub.status.busy": "2026-01-16T09:10:33.498149Z",
     "iopub.status.idle": "2026-01-16T09:10:41.447481Z",
     "shell.execute_reply": "2026-01-16T09:10:41.446591Z",
     "shell.execute_reply.started": "2026-01-16T09:10:33.498397Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda | GPUs: 2\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import os\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# -----------------\n",
    "# Speed preset\n",
    "# -----------------\n",
    "FAST_MODE = True  # set False for max quality\n",
    "\n",
    "# Device / seed\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device, '| GPUs:', torch.cuda.device_count())\n",
    "\n",
    "# GPU perf toggles (help a lot on T4)\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "    try:\n",
    "        torch.set_float32_matmul_precision('high')\n",
    "    except Exception:\n",
    "        pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T09:10:41.449641Z",
     "iopub.status.busy": "2026-01-16T09:10:41.449239Z",
     "iopub.status.idle": "2026-01-16T09:11:51.618927Z",
     "shell.execute_reply": "2026-01-16T09:11:51.618184Z",
     "shell.execute_reply.started": "2026-01-16T09:10:41.449615Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading MNIST datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9.91M/9.91M [00:00<00:00, 10.4MB/s]\n",
      "100%|██████████| 28.9k/28.9k [00:00<00:00, 340kB/s]\n",
      "100%|██████████| 1.65M/1.65M [00:00<00:00, 3.20MB/s]\n",
      "100%|██████████| 4.54k/4.54k [00:00<00:00, 8.64MB/s]\n",
      "100%|██████████| 26.4M/26.4M [00:00<00:00, 113MB/s]\n",
      "100%|██████████| 29.5k/29.5k [00:00<00:00, 3.71MB/s]\n",
      "100%|██████████| 4.42M/4.42M [00:00<00:00, 66.5MB/s]\n",
      "100%|██████████| 5.15k/5.15k [00:00<00:00, 12.1MB/s]\n",
      "100%|██████████| 18.2M/18.2M [00:28<00:00, 647kB/s]\n",
      "100%|██████████| 29.5k/29.5k [00:00<00:00, 130kB/s]\n",
      "100%|██████████| 3.04M/3.04M [00:02<00:00, 1.04MB/s]\n",
      "100%|██████████| 5.12k/5.12k [00:00<00:00, 11.2MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ KMNIST loaded: 60000 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 562M/562M [00:02<00:00, 242MB/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ EMNIST loaded: 697932 samples\n",
      "✗ QMNIST not available (may need: pip install qmnist): No module named 'qmnist'\n",
      "\n",
      "✓ Total dataset size: 877932 samples | batch_size: 512\n",
      "  - MNIST: 60000\n",
      "  - Fashion-MNIST: 60000\n",
      "  - KMNIST: 60000\n",
      "  - EMNIST: 697932\n"
     ]
    }
   ],
   "source": [
    "# Data: ALL MNIST-style datasets with text prompts\n",
    "# We train on (image, prompt) pairs so the model learns real text conditioning.\n",
    "\n",
    "MNIST_NAMES = [\n",
    "    \"zero\", \"one\", \"two\", \"three\", \"four\",\n",
    "    \"five\", \"six\", \"seven\", \"eight\", \"nine\",\n",
    "]\n",
    "FASHION_NAMES = [\n",
    "    \"t-shirt/top\", \"trouser\", \"pullover\", \"dress\", \"coat\",\n",
    "    \"sandal\", \"shirt\", \"sneaker\", \"bag\", \"ankle boot\",\n",
    "]\n",
    "# KMNIST (Kuzushiji-MNIST) - Japanese cursive characters\n",
    "KMNIST_NAMES = [\n",
    "    \"o\", \"ki\", \"su\", \"tsu\", \"na\", \"ha\", \"ma\", \"ya\", \"re\", \"wo\"\n",
    "]\n",
    "# EMNIST Letters (A-Z, a-z) - we'll use ByClass split\n",
    "EMNIST_LETTERS = [chr(i) for i in range(ord('A'), ord('Z')+1)] + [chr(i) for i in range(ord('a'), ord('z')+1)]\n",
    "\n",
    "MNIST_TEMPLATES = [\n",
    "    \"digit {name}\",\n",
    "    \"handwritten digit {name}\",\n",
    "    \"number {name}\",\n",
    "]\n",
    "FASHION_TEMPLATES = [\n",
    "    \"fashion {name}\",\n",
    "    \"clothing {name}\",\n",
    "    \"apparel {name}\",\n",
    "]\n",
    "KMNIST_TEMPLATES = [\n",
    "    \"japanese character {name}\",\n",
    "    \"kuzushiji {name}\",\n",
    "    \"cursive {name}\",\n",
    "]\n",
    "EMNIST_TEMPLATES = [\n",
    "    \"letter {name}\",\n",
    "    \"character {name}\",\n",
    "    \"alphabet {name}\",\n",
    "]\n",
    "\n",
    "def prompt_mnist(y: int) -> str:\n",
    "    name = MNIST_NAMES[int(y)]\n",
    "    return random.choice(MNIST_TEMPLATES).format(name=name)\n",
    "\n",
    "def prompt_fashion(y: int) -> str:\n",
    "    name = FASHION_NAMES[int(y)]\n",
    "    return random.choice(FASHION_TEMPLATES).format(name=name)\n",
    "\n",
    "def prompt_kmnist(y: int) -> str:\n",
    "    name = KMNIST_NAMES[int(y)]\n",
    "    return random.choice(KMNIST_TEMPLATES).format(name=name)\n",
    "\n",
    "def prompt_emnist(y: int) -> str:\n",
    "    # EMNIST ByClass has 62 classes (A-Z, a-z, 0-9)\n",
    "    # For simplicity, map to letters if available\n",
    "    if y < len(EMNIST_LETTERS):\n",
    "        name = EMNIST_LETTERS[int(y)]\n",
    "    else:\n",
    "        # Fallback to digit names for 0-9\n",
    "        digit_idx = y - len(EMNIST_LETTERS)\n",
    "        if 0 <= digit_idx < 10:\n",
    "            name = MNIST_NAMES[digit_idx]\n",
    "        else:\n",
    "            name = f\"class_{y}\"\n",
    "    return random.choice(EMNIST_TEMPLATES).format(name=name)\n",
    "\n",
    "# Enhanced transform with data augmentation\n",
    "transform_base = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,)),  # [-1,1]\n",
    "])\n",
    "\n",
    "# Augmented transform for training (improves generalization)\n",
    "# Reduced augmentation strength for better quality - too much can hurt 28x28 images\n",
    "transform_aug = transforms.Compose([\n",
    "    transforms.RandomAffine(degrees=5, translate=(0.05, 0.05), scale=(0.95, 1.05)),  # Reduced from 10/0.1/0.9-1.1\n",
    "    transforms.RandomRotation(degrees=3),  # Reduced from 5\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,)),  # [-1,1]\n",
    "])\n",
    "\n",
    "class PromptedDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, base_ds, prompt_fn, use_augmentation=False):\n",
    "        self.base_ds = base_ds\n",
    "        self.prompt_fn = prompt_fn\n",
    "        self.use_augmentation = use_augmentation\n",
    "    def __len__(self):\n",
    "        return len(self.base_ds)\n",
    "    def __getitem__(self, idx):\n",
    "        x, y = self.base_ds[idx]\n",
    "        # Apply augmentation if enabled (randomly)\n",
    "        # Note: base_ds already applies transform_base, so x is a tensor in [-1, 1]\n",
    "        if self.use_augmentation and random.random() < 0.5:\n",
    "            # Denormalize to [0, 1] for PIL conversion\n",
    "            x_denorm = (x + 1.0) / 2.0\n",
    "            x_denorm = torch.clamp(x_denorm, 0, 1)\n",
    "            x_pil = transforms.ToPILImage()(x_denorm)\n",
    "            # Apply augmentation (includes normalization back to [-1, 1])\n",
    "            x = transform_aug(x_pil)\n",
    "        # x is already normalized from base_ds transform\n",
    "        return x, self.prompt_fn(y)\n",
    "\n",
    "# Load all available MNIST-style datasets\n",
    "print(\"Loading MNIST datasets...\")\n",
    "mnist_train = datasets.MNIST(root='./data', train=True, download=True, transform=transform_base)\n",
    "fashion_train = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform_base)\n",
    "\n",
    "# Try to load KMNIST (Kuzushiji-MNIST)\n",
    "try:\n",
    "    kmnist_train = datasets.KMNIST(root='./data', train=True, download=True, transform=transform_base)\n",
    "    print(f\"✓ KMNIST loaded: {len(kmnist_train)} samples\")\n",
    "    kmnist_available = True\n",
    "except Exception as e:\n",
    "    print(f\"✗ KMNIST not available: {e}\")\n",
    "    kmnist_available = False\n",
    "\n",
    "# Try to load EMNIST (Extended MNIST)\n",
    "try:\n",
    "    emnist_train = datasets.EMNIST(root='./data', split='byclass', train=True, download=True, transform=transform_base)\n",
    "    print(f\"✓ EMNIST loaded: {len(emnist_train)} samples\")\n",
    "    emnist_available = True\n",
    "except Exception as e:\n",
    "    print(f\"✗ EMNIST not available: {e}\")\n",
    "    emnist_available = False\n",
    "\n",
    "# Try to load QMNIST (if available via custom loader)\n",
    "qmnist_available = False\n",
    "try:\n",
    "    # QMNIST might need special handling - try importing\n",
    "    import qmnist\n",
    "    qmnist_train = qmnist.QMNIST(root='./data', train=True, download=True, transform=transform_base)\n",
    "    print(f\"✓ QMNIST loaded: {len(qmnist_train)} samples\")\n",
    "    qmnist_available = True\n",
    "except Exception as e:\n",
    "    print(f\"✗ QMNIST not available (may need: pip install qmnist): {e}\")\n",
    "    qmnist_available = False\n",
    "\n",
    "# Combine all available datasets\n",
    "datasets_list = [\n",
    "    PromptedDataset(mnist_train, prompt_mnist, use_augmentation=True),\n",
    "    PromptedDataset(fashion_train, prompt_fashion, use_augmentation=True),\n",
    "]\n",
    "\n",
    "if kmnist_available:\n",
    "    datasets_list.append(PromptedDataset(kmnist_train, prompt_kmnist, use_augmentation=True))\n",
    "\n",
    "if emnist_available:\n",
    "    datasets_list.append(PromptedDataset(emnist_train, prompt_emnist, use_augmentation=True))\n",
    "\n",
    "train_ds = torch.utils.data.ConcatDataset(datasets_list)\n",
    "\n",
    "batch_size = (512 if FAST_MODE else 256) if torch.cuda.is_available() else 128\n",
    "print(f'\\n✓ Total dataset size: {len(train_ds)} samples | batch_size: {batch_size}')\n",
    "print(f'  - MNIST: {len(mnist_train)}')\n",
    "print(f'  - Fashion-MNIST: {len(fashion_train)}')\n",
    "if kmnist_available:\n",
    "    print(f'  - KMNIST: {len(kmnist_train)}')\n",
    "if emnist_available:\n",
    "    print(f'  - EMNIST: {len(emnist_train)}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T09:12:47.765450Z",
     "iopub.status.busy": "2026-01-16T09:12:47.764742Z",
     "iopub.status.idle": "2026-01-16T09:12:47.793962Z",
     "shell.execute_reply": "2026-01-16T09:12:47.793139Z",
     "shell.execute_reply.started": "2026-01-16T09:12:47.765425Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Building blocks: time embedding, text encoder, FiLM-UNet\n",
    "\n",
    "@dataclass\n",
    "class DiffusionConfig:\n",
    "    img_size: int = 28\n",
    "\n",
    "    # model - improved architecture\n",
    "    base_channels: int = 32  # FAST default (64 for quality mode)\n",
    "    channel_mults: tuple = (1, 2, 4)  # Increased depth for better capacity\n",
    "    text_dim: int = 128\n",
    "    time_dim: int = 128\n",
    "    use_attn: bool = False  # Enable in quality mode\n",
    "    attn_heads: int = 4\n",
    "    num_layers_text: int = 2\n",
    "    dropout: float = 0.1\n",
    "\n",
    "    # diffusion - improved schedule\n",
    "    timesteps: int = 200\n",
    "    schedule: str = 'cosine'  # 'cosine' | 'linear'\n",
    "    beta_start: float = 1e-4\n",
    "    beta_end: float = 0.02\n",
    "    # Better noise schedule parameters for cleaner generation\n",
    "\n",
    "def sinusoidal_time_embedding(timesteps: torch.Tensor, dim: int) -> torch.Tensor:\n",
    "    half = dim // 2\n",
    "    freqs = torch.exp(-math.log(10000) * torch.arange(half, device=timesteps.device) / (half - 1))\n",
    "    args = timesteps[:, None] * freqs[None, :]\n",
    "    emb = torch.cat([torch.sin(args), torch.cos(args)], dim=-1)\n",
    "    return emb\n",
    "\n",
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size: int, emb_dim: int, hidden_dim: int, num_layers: int = 2, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.gru = nn.GRU(emb_dim, hidden_dim, num_layers=num_layers, batch_first=True, bidirectional=True, dropout=dropout)\n",
    "        self.proj = nn.Sequential(nn.Linear(hidden_dim * 2, hidden_dim), nn.SiLU(), nn.Linear(hidden_dim, hidden_dim))\n",
    "    def forward(self, tokens, lengths):\n",
    "        x = self.embedding(tokens)\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(x, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        out, _ = self.gru(packed)\n",
    "        out, _ = nn.utils.rnn.pad_packed_sequence(out, batch_first=True)\n",
    "        # take last valid timestep per sequence\n",
    "        idx = (lengths - 1).clamp(min=0)\n",
    "        last = out[torch.arange(out.size(0)), idx]\n",
    "        return self.proj(last)\n",
    "\n",
    "class FiLM(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(in_dim, out_dim * 2)\n",
    "    def forward(self, x, cond):\n",
    "        scale, shift = self.linear(cond).chunk(2, dim=1)\n",
    "        return x * (1 + scale[:, :, None, None]) + shift[:, :, None, None]\n",
    "\n",
    "class SelfAttention2d(nn.Module):\n",
    "    def __init__(self, channels: int, heads: int = 4):\n",
    "        super().__init__()\n",
    "        assert channels % heads == 0, 'channels must be divisible by heads'\n",
    "        self.heads = heads\n",
    "        self.norm = nn.GroupNorm(8, channels)\n",
    "        self.qkv = nn.Conv2d(channels, channels * 3, 1)\n",
    "        self.proj = nn.Conv2d(channels, channels, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, h, w = x.shape\n",
    "        x_in = x\n",
    "        x = self.norm(x)\n",
    "        qkv = self.qkv(x)\n",
    "        q, k, v = qkv.chunk(3, dim=1)\n",
    "\n",
    "        # (B, heads, C//heads, HW)\n",
    "        q = q.view(b, self.heads, c // self.heads, h * w)\n",
    "        k = k.view(b, self.heads, c // self.heads, h * w)\n",
    "        v = v.view(b, self.heads, c // self.heads, h * w)\n",
    "\n",
    "        q = q.permute(0, 1, 3, 2)  # (B, heads, HW, d)\n",
    "        k = k.permute(0, 1, 2, 3)  # (B, heads, d, HW)\n",
    "        attn = (q @ k) / math.sqrt(c // self.heads)\n",
    "        attn = attn.softmax(dim=-1)\n",
    "\n",
    "        v = v.permute(0, 1, 3, 2)  # (B, heads, HW, d)\n",
    "        out = attn @ v\n",
    "        out = out.permute(0, 1, 3, 2).contiguous().view(b, c, h, w)\n",
    "        out = self.proj(out)\n",
    "        return x_in + out\n",
    "\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, time_dim, text_dim):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_ch, out_ch, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, padding=1)\n",
    "        self.time_film = FiLM(time_dim, out_ch)\n",
    "        self.text_film = FiLM(text_dim, out_ch)\n",
    "        self.act = nn.SiLU()\n",
    "        self.norm1 = nn.GroupNorm(8, out_ch)\n",
    "        self.norm2 = nn.GroupNorm(8, out_ch)\n",
    "        self.skip = nn.Conv2d(in_ch, out_ch, 1) if in_ch != out_ch else nn.Identity()\n",
    "\n",
    "    def forward(self, x, t_emb, txt_emb):\n",
    "        h = self.act(self.norm1(self.conv1(x)))\n",
    "        h = self.time_film(h, t_emb)\n",
    "        h = self.text_film(h, txt_emb)\n",
    "        h = self.act(self.norm2(self.conv2(h)))\n",
    "        return h + self.skip(x)\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, cfg: DiffusionConfig, text_vocab: int):\n",
    "        super().__init__()\n",
    "        ch = cfg.base_channels\n",
    "        self.text_encoder = TextEncoder(\n",
    "            text_vocab,\n",
    "            emb_dim=cfg.text_dim,\n",
    "            hidden_dim=cfg.text_dim,\n",
    "            num_layers=cfg.num_layers_text,\n",
    "            dropout=cfg.dropout,\n",
    "        )\n",
    "        self.null_text = nn.Parameter(torch.zeros(cfg.text_dim))\n",
    "\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            nn.Linear(cfg.time_dim, cfg.time_dim * 4), nn.SiLU(), nn.Linear(cfg.time_dim * 4, cfg.time_dim)\n",
    "        )\n",
    "\n",
    "        Attn = (lambda c: SelfAttention2d(c, heads=cfg.attn_heads)) if cfg.use_attn else (lambda c: nn.Identity())\n",
    "\n",
    "        # Down - improved with more layers\n",
    "        self.enc1 = ResBlock(1, ch, cfg.time_dim, cfg.text_dim)\n",
    "        self.enc2 = ResBlock(ch, ch * cfg.channel_mults[1], cfg.time_dim, cfg.text_dim)\n",
    "        self.enc2_attn = Attn(ch * cfg.channel_mults[1])\n",
    "        self.enc3 = ResBlock(ch * cfg.channel_mults[1], ch * cfg.channel_mults[2], cfg.time_dim, cfg.text_dim)\n",
    "        self.enc3_attn = Attn(ch * cfg.channel_mults[2])\n",
    "        self.pool = nn.AvgPool2d(2)\n",
    "\n",
    "        # Bottleneck - deeper\n",
    "        self.mid1 = ResBlock(ch * cfg.channel_mults[2], ch * cfg.channel_mults[2], cfg.time_dim, cfg.text_dim)\n",
    "        self.mid_attn = Attn(ch * cfg.channel_mults[2])\n",
    "        self.mid2 = ResBlock(ch * cfg.channel_mults[2], ch * cfg.channel_mults[2], cfg.time_dim, cfg.text_dim)\n",
    "\n",
    "        # Up - improved skip connections\n",
    "        self.up1 = nn.ConvTranspose2d(ch * cfg.channel_mults[2], ch * cfg.channel_mults[1], 2, stride=2)\n",
    "        self.dec1 = ResBlock(ch * cfg.channel_mults[1] * 2, ch * cfg.channel_mults[1], cfg.time_dim, cfg.text_dim)\n",
    "        self.dec1_attn = Attn(ch * cfg.channel_mults[1])\n",
    "        self.up2 = nn.ConvTranspose2d(ch * cfg.channel_mults[1], ch, 2, stride=2)\n",
    "        self.dec2 = ResBlock(ch * 2, ch, cfg.time_dim, cfg.text_dim)\n",
    "\n",
    "        self.out = nn.Conv2d(ch, 1, 1)\n",
    "\n",
    "    def forward(self, x, t, txt_tokens, txt_lens, drop_text_prob: float = 0.1):\n",
    "        t_emb = self.time_mlp(sinusoidal_time_embedding(t, self.time_mlp[0].in_features))\n",
    "\n",
    "        # --- classifier-free guidance support ---\n",
    "        # During sampling we need a true unconditional path even in eval(),\n",
    "        # so drop_text_prob==1.0 forces the null embedding.\n",
    "        if drop_text_prob >= 1.0:\n",
    "            txt_emb = self.null_text[None, :].expand(x.size(0), -1)\n",
    "        else:\n",
    "            txt_emb = self.text_encoder(txt_tokens, txt_lens)\n",
    "            if self.training and drop_text_prob > 0.0:\n",
    "                mask = (torch.rand(txt_emb.size(0), device=txt_emb.device) < drop_text_prob).float()[:, None]\n",
    "                txt_emb = txt_emb * (1 - mask) + self.null_text[None, :] * mask\n",
    "\n",
    "        e1 = self.enc1(x, t_emb, txt_emb)\n",
    "        e2 = self.enc2(self.pool(e1), t_emb, txt_emb)\n",
    "        e2 = self.enc2_attn(e2)\n",
    "        e3 = self.enc3(self.pool(e2), t_emb, txt_emb)\n",
    "        e3 = self.enc3_attn(e3)\n",
    "\n",
    "        m = self.mid1(e3, t_emb, txt_emb)\n",
    "        m = self.mid_attn(m)\n",
    "        m = self.mid2(m, t_emb, txt_emb)\n",
    "\n",
    "        d1 = self.up1(m)\n",
    "        d1 = torch.cat([d1, e2], dim=1)\n",
    "        d1 = self.dec1(d1, t_emb, txt_emb)\n",
    "        d1 = self.dec1_attn(d1)\n",
    "        d2 = self.up2(d1)\n",
    "        d2 = torch.cat([d2, e1], dim=1)\n",
    "        d2 = self.dec2(d2, t_emb, txt_emb)\n",
    "        return self.out(d2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T09:12:53.867975Z",
     "iopub.status.busy": "2026-01-16T09:12:53.867411Z",
     "iopub.status.idle": "2026-01-16T09:12:53.878192Z",
     "shell.execute_reply": "2026-01-16T09:12:53.877307Z",
     "shell.execute_reply.started": "2026-01-16T09:12:53.867948Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 31\n"
     ]
    }
   ],
   "source": [
    "# Tokenizer helpers (simple character-level; robust for small prompt vocab)\n",
    "\n",
    "class SimpleCharTokenizer:\n",
    "    def __init__(self, texts, pad_token='<pad>', unk_token='<unk>'):\n",
    "        chars = sorted(list({c for t in texts for c in t.lower()}))\n",
    "        self.pad_token = pad_token\n",
    "        self.unk_token = unk_token\n",
    "        self.itos = [pad_token, unk_token] + chars\n",
    "        self.stoi = {c: i for i, c in enumerate(self.itos)}\n",
    "\n",
    "    @property\n",
    "    def vocab_size(self):\n",
    "        return len(self.itos)\n",
    "\n",
    "    def encode(self, text: str, max_len: int = 32):\n",
    "        text = text.lower()\n",
    "        ids = [self.stoi.get(c, self.stoi[self.unk_token]) for c in text[:max_len]]\n",
    "        length = len(ids)\n",
    "        if length < max_len:\n",
    "            ids += [self.stoi[self.pad_token]] * (max_len - length)\n",
    "        return torch.tensor(ids, dtype=torch.long), torch.tensor(length, dtype=torch.long)\n",
    "\n",
    "    def encode_batch(self, texts, max_len: int = 32):\n",
    "        toks, lens = zip(*[self.encode(t, max_len=max_len) for t in texts])\n",
    "        return torch.stack(toks), torch.stack(lens)\n",
    "\n",
    "# Build tokenizer vocab from all datasets' templates\n",
    "all_prompts = []\n",
    "# MNIST prompts\n",
    "for i in range(10):\n",
    "    for tpl in MNIST_TEMPLATES:\n",
    "        all_prompts.append(tpl.format(name=MNIST_NAMES[i]))\n",
    "    for tpl in FASHION_TEMPLATES:\n",
    "        all_prompts.append(tpl.format(name=FASHION_NAMES[i]))\n",
    "# KMNIST prompts (if available)\n",
    "if 'kmnist_available' in globals() and kmnist_available:\n",
    "    for i in range(10):\n",
    "        for tpl in KMNIST_TEMPLATES:\n",
    "            all_prompts.append(tpl.format(name=KMNIST_NAMES[i]))\n",
    "# EMNIST prompts (if available)\n",
    "if 'emnist_available' in globals() and emnist_available:\n",
    "    for letter in EMNIST_LETTERS[:26]:  # A-Z\n",
    "        for tpl in EMNIST_TEMPLATES:\n",
    "            all_prompts.append(tpl.format(name=letter))\n",
    "\n",
    "tokenizer = SimpleCharTokenizer(all_prompts)\n",
    "print('Vocab size:', tokenizer.vocab_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T09:12:57.034741Z",
     "iopub.status.busy": "2026-01-16T09:12:57.034049Z",
     "iopub.status.idle": "2026-01-16T09:12:57.054550Z",
     "shell.execute_reply": "2026-01-16T09:12:57.053911Z",
     "shell.execute_reply.started": "2026-01-16T09:12:57.034710Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train batches: 1715 | num_workers: 4\n"
     ]
    }
   ],
   "source": [
    "# Diffusion utilities\n",
    "\n",
    "def cosine_beta_schedule(timesteps: int, s: float = 0.008):\n",
    "    # From Nichol & Dhariwal 2021 (Improved DDPM)\n",
    "    # Improved with better s parameter for smoother transitions\n",
    "    steps = timesteps + 1\n",
    "    x = torch.linspace(0, timesteps, steps, dtype=torch.float32)\n",
    "    alphas_cum = torch.cos(((x / timesteps) + s) / (1 + s) * math.pi * 0.5) ** 2\n",
    "    alphas_cum = alphas_cum / alphas_cum[0]\n",
    "    betas = 1 - (alphas_cum[1:] / alphas_cum[:-1])\n",
    "    # Clamp to reasonable range to avoid numerical issues\n",
    "    return betas.clamp(1e-5, 0.999)\n",
    "\n",
    "\n",
    "class SimpleDiffusion(nn.Module):\n",
    "    \"\"\"DDPM/DDIM utilities with schedule tensors registered as buffers (so .to(device) works).\"\"\"\n",
    "    def __init__(self, cfg: DiffusionConfig):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "\n",
    "        if cfg.schedule == 'cosine':\n",
    "            betas = cosine_beta_schedule(cfg.timesteps)\n",
    "        else:\n",
    "            betas = torch.linspace(cfg.beta_start, cfg.beta_end, cfg.timesteps, dtype=torch.float32)\n",
    "\n",
    "        alphas = 1.0 - betas\n",
    "        alphas_cum = torch.cumprod(alphas, dim=0)\n",
    "\n",
    "        self.register_buffer('betas', betas)\n",
    "        self.register_buffer('alphas', alphas)\n",
    "        self.register_buffer('alphas_cum', alphas_cum)\n",
    "\n",
    "    def sample_timesteps(self, batch_size: int, device: Optional[torch.device] = None):\n",
    "        if device is None:\n",
    "            device = self.betas.device\n",
    "        return torch.randint(0, self.cfg.timesteps, (batch_size,), device=device)\n",
    "\n",
    "    def add_noise(self, x0, t, noise):\n",
    "        # t: (B,) long on same device as buffers\n",
    "        sqrt_ac = self.alphas_cum[t].sqrt()[:, None, None, None]\n",
    "        sqrt_one_minus_ac = (1 - self.alphas_cum[t]).sqrt()[:, None, None, None]\n",
    "        return sqrt_ac * x0 + sqrt_one_minus_ac * noise\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _predict_eps_cfg(self, model, x, t: int, txt_tokens, txt_lens, guidance_scale: float):\n",
    "        # classifier-free guidance\n",
    "        t_batch = torch.full((x.size(0),), t, device=x.device, dtype=torch.long)\n",
    "        eps_text = model(x, t_batch, txt_tokens, txt_lens, drop_text_prob=0.0)\n",
    "        eps_null = model(x, t_batch, txt_tokens, txt_lens, drop_text_prob=1.0)\n",
    "        return eps_null + guidance_scale * (eps_text - eps_null)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def p_sample(self, model, x, t: int, txt_tokens, txt_lens, guidance_scale: float = 2.0):\n",
    "        \"\"\"Ancestral DDPM step (kept for reference).\"\"\"\n",
    "        eps = self._predict_eps_cfg(model, x, t, txt_tokens, txt_lens, guidance_scale)\n",
    "        beta_t = self.betas[t]\n",
    "        alpha_t = self.alphas[t]\n",
    "        alpha_cum_t = self.alphas_cum[t]\n",
    "        mean = (1 / alpha_t.sqrt()) * (x - beta_t / (1 - alpha_cum_t).sqrt() * eps)\n",
    "        if t == 0:\n",
    "            return mean\n",
    "        noise = torch.randn_like(x)\n",
    "        return mean + beta_t.sqrt() * noise\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def ddim_step(self, model, x, t: int, t_prev: int, txt_tokens, txt_lens, guidance_scale: float = 2.0, eta: float = 0.0):\n",
    "        \"\"\"DDIM step that supports skipping timesteps cleanly.\"\"\"\n",
    "        eps = self._predict_eps_cfg(model, x, t, txt_tokens, txt_lens, guidance_scale)\n",
    "\n",
    "        ac_t = self.alphas_cum[t]\n",
    "        ac_prev = self.alphas_cum[t_prev] if t_prev >= 0 else torch.tensor(1.0, device=x.device)\n",
    "\n",
    "        # predict x0\n",
    "        x0 = (x - (1 - ac_t).sqrt() * eps) / ac_t.sqrt()\n",
    "        x0 = x0.clamp(-1, 1)\n",
    "\n",
    "        # DDIM variance control\n",
    "        if t_prev < 0:\n",
    "            return x0\n",
    "\n",
    "        sigma = eta * torch.sqrt((1 - ac_prev) / (1 - ac_t) * (1 - ac_t / ac_prev))\n",
    "        noise = torch.randn_like(x) if eta > 0 else torch.zeros_like(x)\n",
    "\n",
    "        dir_xt = (1 - ac_prev - sigma**2).sqrt() * eps\n",
    "        x_prev = ac_prev.sqrt() * x0 + dir_xt + sigma * noise\n",
    "        return x_prev\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sample(self, model, txt_tokens, txt_lens, steps: int = 40, guidance_scale: float = 2.0, eta: float = 0.0):\n",
    "        model.eval()\n",
    "        b = txt_tokens.size(0)\n",
    "        x = torch.randn(b, 1, self.cfg.img_size, self.cfg.img_size, device=txt_tokens.device)\n",
    "\n",
    "        # choose a schedule of timesteps (descending)\n",
    "        steps = int(steps)\n",
    "        steps = max(2, min(steps, self.cfg.timesteps))\n",
    "        ts = torch.linspace(self.cfg.timesteps - 1, 0, steps, device=txt_tokens.device).long()\n",
    "\n",
    "        for i in range(len(ts)):\n",
    "            t = int(ts[i].item())\n",
    "            t_prev = int(ts[i + 1].item()) if i + 1 < len(ts) else -1\n",
    "            x = self.ddim_step(model, x, t, t_prev, txt_tokens, txt_lens, guidance_scale=guidance_scale, eta=eta)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "def collate_batch(batch):\n",
    "    imgs, prompts = zip(*batch)\n",
    "    imgs = torch.stack(imgs)\n",
    "    toks, lens = tokenizer.encode_batch(prompts, max_len=32)\n",
    "    return imgs, toks, lens, list(prompts)\n",
    "\n",
    "# DataLoader (now that tokenizer + collate exist)\n",
    "num_workers = 4 if torch.cuda.is_available() else 0\n",
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=(num_workers > 0),\n",
    "    prefetch_factor=4 if num_workers > 0 else None,\n",
    "    collate_fn=collate_batch,\n",
    ")\n",
    "print('Train batches:', len(train_loader), '| num_workers:', num_workers)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T09:13:01.292596Z",
     "iopub.status.busy": "2026-01-16T09:13:01.291831Z",
     "iopub.status.idle": "2026-01-16T09:44:50.326936Z",
     "shell.execute_reply": "2026-01-16T09:44:50.325046Z",
     "shell.execute_reply.started": "2026-01-16T09:13:01.292566Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: DiffusionConfig(img_size=28, base_channels=32, channel_mults=(1, 2, 4), text_dim=128, time_dim=128, use_attn=False, attn_heads=4, num_layers_text=2, dropout=0.1, timesteps=200, schedule='cosine', beta_start=0.0001, beta_end=0.02)\n",
      "torch.compile enabled\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_55/3006531319.py:77: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=torch.cuda.is_available())\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0399d47f91b84822be2d4956b2c7bc1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/15000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_55/3006531319.py:114: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n",
      "W0116 09:13:20.278000 55 torch/_inductor/utils.py:1436] [3/0] Not enough SMs to use max_autotune_gemm mode\n",
      "/usr/local/lib/python3.12/dist-packages/torch/optim/lr_scheduler.py:192: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_55/3006531319.py:114: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n",
      "/tmp/ipykernel_55/3006531319.py:114: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training steps: 15000\n"
     ]
    }
   ],
   "source": [
    "# Model + training loop\n",
    "\n",
    "cfg = DiffusionConfig()\n",
    "\n",
    "# FAST/quality presets - improved for multi-dataset training\n",
    "if not FAST_MODE:\n",
    "    cfg.base_channels = 64\n",
    "    cfg.use_attn = True\n",
    "    cfg.timesteps = 500  # More timesteps for better quality\n",
    "    cfg.channel_mults = (1, 2, 4, 4)  # Even deeper for quality mode\n",
    "else:\n",
    "    cfg.base_channels = 32\n",
    "    cfg.use_attn = False\n",
    "    cfg.timesteps = 300  # Increased from 200 for better quality even in fast mode\n",
    "    cfg.channel_mults = (1, 2, 4)  # Improved from (1, 2, 2)\n",
    "\n",
    "print('Config:', cfg)\n",
    "\n",
    "diffusion = SimpleDiffusion(cfg).to(device)\n",
    "\n",
    "# Note: for this tiny 28×28 model, nn.DataParallel often *slows down*.\n",
    "# We'll default to single-GPU fast path; you can force DataParallel if you want.\n",
    "USE_DATAPARALLEL = False\n",
    "\n",
    "base_model = UNet(cfg, text_vocab=tokenizer.vocab_size).to(device)\n",
    "\n",
    "if USE_DATAPARALLEL and torch.cuda.device_count() > 1:\n",
    "    print('Using DataParallel on', torch.cuda.device_count(), 'GPUs')\n",
    "    model = nn.DataParallel(base_model)\n",
    "else:\n",
    "    model = base_model\n",
    "\n",
    "# channels_last can speed convs on some GPUs\n",
    "if torch.cuda.is_available():\n",
    "    model = model.to(memory_format=torch.channels_last)\n",
    "\n",
    "# optional torch.compile (PyTorch 2.x) - can speed up steady-state\n",
    "USE_COMPILE = FAST_MODE\n",
    "if USE_COMPILE and hasattr(torch, 'compile'):\n",
    "    try:\n",
    "        model = torch.compile(model)\n",
    "        print('torch.compile enabled')\n",
    "    except Exception as e:\n",
    "        print('torch.compile failed:', e)\n",
    "\n",
    "# Improved optimizer with learning rate scheduling\n",
    "# Improved learning rate - slightly lower for better stability with multiple datasets\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(), \n",
    "    lr=1.5e-4 if FAST_MODE else 8e-5,  # Reduced LR for better convergence\n",
    "    weight_decay=1e-2,\n",
    "    betas=(0.9, 0.999),\n",
    "    eps=1e-8\n",
    ")\n",
    "\n",
    "# EMA helps samples look cleaner with the same number of training steps\n",
    "@torch.no_grad()\n",
    "def unwrap(m: nn.Module) -> nn.Module:\n",
    "    # Handle torch.compile (stores original in _orig_mod)\n",
    "    if hasattr(m, '_orig_mod'):\n",
    "        m = m._orig_mod\n",
    "    # Handle DataParallel (stores original in .module)\n",
    "    if hasattr(m, 'module'):\n",
    "        m = m.module\n",
    "    return m\n",
    "\n",
    "@torch.no_grad()\n",
    "def ema_update(ema_model: nn.Module, model: nn.Module, decay: float = 0.9995):\n",
    "    src = unwrap(model)\n",
    "    for ema_p, p in zip(ema_model.parameters(), src.parameters()):\n",
    "        ema_p.data.mul_(decay).add_(p.data, alpha=1 - decay)\n",
    "\n",
    "ema_model = UNet(cfg, text_vocab=tokenizer.vocab_size).to(device)\n",
    "ema_model.load_state_dict(unwrap(model).state_dict())\n",
    "# Slightly lower EMA decay for faster adaptation during training\n",
    "ema_decay = 0.9995  # Increased from 0.999 for better quality\n",
    "\n",
    "# Mixed precision for speed on T4 (fixed deprecated API)\n",
    "if torch.cuda.is_available():\n",
    "    scaler = torch.amp.GradScaler('cuda')\n",
    "else:\n",
    "    scaler = None\n",
    "\n",
    "# Increased training steps for better convergence with multiple datasets\n",
    "# More steps needed for better quality with multiple diverse datasets\n",
    "num_steps = 20_000 if FAST_MODE else 80_000\n",
    "log_interval = 200\n",
    "\n",
    "# Learning rate scheduler for better convergence (initialized after num_steps)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer, \n",
    "    T_max=num_steps,\n",
    "    eta_min=1e-6\n",
    ")\n",
    "\n",
    "# optional gradient accumulation (lets you use big effective batch without huge VRAM)\n",
    "grad_accum = 1 if FAST_MODE else 2\n",
    "\n",
    "model.train()\n",
    "step = 0\n",
    "pbar = tqdm(total=num_steps, desc='train')\n",
    "while step < num_steps:\n",
    "    for x, tokens, lens, _prompts in train_loader:\n",
    "        x = x.to(device, non_blocking=True)\n",
    "        tokens = tokens.to(device, non_blocking=True)\n",
    "        lens = lens.to(device, non_blocking=True)\n",
    "\n",
    "        # channels_last input (must match model memory_format)\n",
    "        if torch.cuda.is_available():\n",
    "            x = x.contiguous(memory_format=torch.channels_last)\n",
    "\n",
    "        t = diffusion.sample_timesteps(x.size(0), device)\n",
    "        noise = torch.randn_like(x)\n",
    "        x_noisy = diffusion.add_noise(x, t, noise)\n",
    "\n",
    "        # gradient accumulation\n",
    "        if step % grad_accum == 0:\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        # Fixed deprecated API - use torch.amp.autocast\n",
    "        autocast_context = torch.amp.autocast('cuda', enabled=torch.cuda.is_available()) if torch.cuda.is_available() else torch.amp.autocast('cpu', enabled=False)\n",
    "        with autocast_context:\n",
    "            pred = model(x_noisy, t, tokens, lens, drop_text_prob=0.1)  # Reduced dropout for better conditioning\n",
    "            # Use Huber loss for more robust training (less sensitive to outliers)\n",
    "            loss = F.smooth_l1_loss(pred, noise, beta=0.1)\n",
    "            loss = loss / grad_accum\n",
    "\n",
    "        if scaler is not None:\n",
    "            scaler.scale(loss).backward()\n",
    "        else:\n",
    "            loss.backward()\n",
    "\n",
    "        if (step + 1) % grad_accum == 0:\n",
    "            if scaler is not None:\n",
    "                scaler.unscale_(optimizer)\n",
    "                nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "            # CRITICAL FIX: scheduler.step() must be called AFTER optimizer.step()\n",
    "            # This ensures the first LR value is not skipped\n",
    "            scheduler.step()\n",
    "            ema_update(ema_model, model, decay=ema_decay)\n",
    "\n",
    "        step += 1\n",
    "        pbar.update(1)\n",
    "        if step % log_interval == 0:\n",
    "            pbar.set_postfix(loss=float(loss.detach()) * grad_accum)\n",
    "        if step >= num_steps:\n",
    "            break\n",
    "pbar.close()\n",
    "print('Finished training steps:', step)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T09:44:50.331436Z",
     "iopub.status.busy": "2026-01-16T09:44:50.329578Z",
     "iopub.status.idle": "2026-01-16T09:44:51.685309Z",
     "shell.execute_reply": "2026-01-16T09:44:51.684591Z",
     "shell.execute_reply.started": "2026-01-16T09:44:50.331366Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1800x200 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Sampling demo\n",
    "# Use EMA weights for nicer samples\n",
    "ema_model.eval()\n",
    "\n",
    "# Demo prompts (all dataset types)\n",
    "prompts = [\n",
    "    \"digit zero\",\n",
    "    \"digit three\",\n",
    "    \"digit seven\",\n",
    "    \"fashion sneaker\",\n",
    "    \"fashion ankle boot\",\n",
    "]\n",
    "# Add prompts for other datasets if available\n",
    "if 'kmnist_available' in globals() and kmnist_available:\n",
    "    prompts.extend([\"japanese character o\", \"kuzushiji ki\"])\n",
    "if 'emnist_available' in globals() and emnist_available:\n",
    "    prompts.extend([\"letter A\", \"letter Z\"])\n",
    "\n",
    "tokens = []\n",
    "lens = []\n",
    "for p in prompts:\n",
    "    tok, l = tokenizer.encode(p)\n",
    "    tokens.append(tok)\n",
    "    lens.append(l)\n",
    "tokens = torch.stack(tokens).to(device)\n",
    "lens = torch.stack(lens).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Improved sampling parameters for better quality\n",
    "    # More steps and higher guidance for clearer results\n",
    "    samples = diffusion.sample(ema_model, tokens, lens, steps=50, guidance_scale=3.0, eta=0.0)\n",
    "\n",
    "imgs = (samples.clamp(-1, 1) * 0.5 + 0.5).cpu()  # back to [0,1]\n",
    "fig, axes = plt.subplots(1, len(prompts), figsize=(len(prompts)*2, 2))\n",
    "for ax, img, p in zip(axes, imgs, prompts):\n",
    "    ax.imshow(img[0], cmap='gray')\n",
    "    ax.axis('off')\n",
    "    ax.set_title(p)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Critical Fixes Applied (v2)\n",
    "\n",
    "### Bug Fixes\n",
    "- ✅ **Fixed scheduler order bug**: `scheduler.step()` now called AFTER `optimizer.step()` (was causing first LR to be skipped)\n",
    "- ✅ **Fixed deprecated APIs**: Updated `torch.cuda.amp.*` to `torch.amp.*` (removes FutureWarnings)\n",
    "- ✅ **Improved loss function**: Changed from MSE to Smooth L1 (Huber loss) for more robust training\n",
    "- ✅ **Reduced classifier-free dropout**: 0.15 → 0.1 for better text conditioning\n",
    "\n",
    "### Training Quality Improvements\n",
    "- ✅ **More training steps**: 20K (fast) / 80K (quality) vs 15K / 60K\n",
    "- ✅ **Better learning rate**: Reduced to 1.5e-4 (fast) / 8e-5 (quality) for stability\n",
    "- ✅ **More diffusion timesteps**: 300 (fast) / 500 (quality) vs 200 / 400\n",
    "- ✅ **Better EMA decay**: 0.9995 vs 0.999 for faster adaptation\n",
    "- ✅ **Reduced data augmentation**: Less aggressive transforms (28x28 images are small)\n",
    "- ✅ **Better sampling**: 50 steps, guidance_scale=3.0 (vs 40 steps, 2.0)\n",
    "\n",
    "## Improvements Made\n",
    "\n",
    "### Multi-Dataset Support\n",
    "- ✓ **MNIST** - Original handwritten digits (60K samples)\n",
    "- ✓ **Fashion-MNIST** - Clothing items (60K samples)\n",
    "- ✓ **KMNIST** - Japanese cursive characters (60K samples, if available)\n",
    "- ✓ **EMNIST** - Extended MNIST with letters (697K samples, if available)\n",
    "- ✓ **QMNIST** - Cleaned/expanded variant (if available via `pip install qmnist`)\n",
    "\n",
    "### Model Architecture Improvements\n",
    "- ✓ Deeper UNet with improved channel multipliers (1, 2, 4) vs (1, 2, 2)\n",
    "- ✓ Additional attention layers in encoder and bottleneck\n",
    "- ✓ Deeper bottleneck with two ResBlocks\n",
    "- ✓ Better skip connections throughout\n",
    "\n",
    "### Training Improvements\n",
    "- ✓ **Data augmentation**: Random affine transforms, rotation, scaling\n",
    "- ✓ **Learning rate scheduling**: Cosine annealing for better convergence\n",
    "- ✓ **Increased training steps**: 15K (fast) / 60K (quality) vs 12K / 50K\n",
    "- ✓ **Improved optimizer**: Better betas and eps settings\n",
    "\n",
    "### Tips / Next steps\n",
    "- Set `FAST_MODE = False` for maximum quality (uses attention, deeper network, more timesteps)\n",
    "- Install additional datasets: `pip install emnist qmnist` for full dataset coverage\n",
    "- Increase `num_steps` and `base_channels` for even better quality\n",
    "- Save/Load: `torch.save(model.state_dict(), 'cfdiffusion.pt')` and load with `model.load_state_dict(torch.load(...))`\n",
    "- Try more diverse prompts by expanding the tokenizer vocabulary\n",
    "- To condition on richer text, swap the GRU for a tiny Transformer encoder\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31236,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
